{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpkYHwCqk7W-"
      },
      "source": [
        "![MuJoCo banner](https://raw.githubusercontent.com/google-deepmind/mujoco/main/banner.png)\n",
        "\n",
        "# <h1><center>Tutorial  <a href=\"https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/mjx/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" width=\"140\" align=\"center\"/></a></center></h1>\n",
        "\n",
        "This notebook provides an introductory tutorial for [**MuJoCo XLA (MJX)**](https://github.com/google-deepmind/mujoco/blob/main/mjx), a JAX-based implementation of MuJoCo useful for RL training workloads.\n",
        "\n",
        "**A Colab runtime with GPU acceleration is required.** If you're using a CPU-only runtime, you can switch using the menu \"Runtime > Change runtime type\".\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBSdkbmGN2K-"
      },
      "source": [
        "### Copyright notice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UbO9uhtBSX5"
      },
      "source": [
        "> <p><small><small>Copyright 2023 DeepMind Technologies Limited.</small></p>\n",
        "> <p><small><small>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at <a href=\"http://www.apache.org/licenses/LICENSE-2.0\">http://www.apache.org/licenses/LICENSE-2.0</a>.</small></small></p>\n",
        "> <p><small><small>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</small></small></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvyGCsgSCxHQ"
      },
      "source": [
        "# Install MuJoCo, MJX, and Brax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "ObF1UXrkb0Nd"
      },
      "outputs": [],
      "source": [
        "#@title Import MuJoCo, MJX, and Brax\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "import functools\n",
        "from IPython.display import HTML\n",
        "import jax\n",
        "from jax import numpy as jp\n",
        "import numpy as np\n",
        "from typing import Any, Dict, Sequence, Tuple, Union\n",
        "\n",
        "from brax import base\n",
        "from brax import envs\n",
        "from brax import math\n",
        "from brax.base import Base, Motion, Transform\n",
        "from brax.envs.base import Env, MjxEnv, State\n",
        "from brax.mjx.base import State as MjxState\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "from brax.training.agents.ppo import networks as ppo_networks\n",
        "from brax.io import html, mjcf, model\n",
        "\n",
        "from etils import epath\n",
        "from flax import struct\n",
        "from matplotlib import pyplot as plt\n",
        "import mediapy as media\n",
        "from ml_collections import config_dict\n",
        "import mujoco\n",
        "from mujoco import mjx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAv6WUVUm78k"
      },
      "source": [
        "# Training a Policy with MJX\n",
        "\n",
        "Running large batch physics simulation is useful for training RL policies. Here we demonstrate training RL policies with MJX using the RL library from [Brax](https://github.com/google/brax).\n",
        "\n",
        "Below, we implement the classic Humanoid environment using MJX and Brax. We inherit from the `MjxEnv` implementation in Brax so that we can step the physics with MJX while training with Brax RL implementations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mtGMYNLE3QJN"
      },
      "outputs": [],
      "source": [
        "#@title Humanoid Env\n",
        "\n",
        "class Humanoid(MjxEnv):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      forward_reward_weight=1.25,\n",
        "      ctrl_cost_weight=0.1,\n",
        "      healthy_reward=5.0,\n",
        "      terminate_when_unhealthy=True,\n",
        "      healthy_z_range=(1.0, 2.0),\n",
        "      reset_noise_scale=1e-2,\n",
        "      exclude_current_positions_from_observation=True,\n",
        "      **kwargs,\n",
        "  ):\n",
        "    path = epath.Path(epath.resource_path('mujoco')) / (\n",
        "        'mjx/benchmark/model/humanoid'\n",
        "    )\n",
        "    mj_model = mujoco.MjModel.from_xml_path(\n",
        "        (path / 'humanoid.xml').as_posix())\n",
        "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
        "    mj_model.opt.iterations = 6\n",
        "    mj_model.opt.ls_iterations = 6\n",
        "\n",
        "    physics_steps_per_control_step = 5\n",
        "    kwargs['n_frames'] = kwargs.get(\n",
        "        'n_frames', physics_steps_per_control_step)\n",
        "\n",
        "    super().__init__(model=mj_model, **kwargs)\n",
        "\n",
        "    self._forward_reward_weight = forward_reward_weight\n",
        "    self._ctrl_cost_weight = ctrl_cost_weight\n",
        "    self._healthy_reward = healthy_reward\n",
        "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
        "    self._healthy_z_range = healthy_z_range\n",
        "    self._reset_noise_scale = reset_noise_scale\n",
        "    self._exclude_current_positions_from_observation = (\n",
        "        exclude_current_positions_from_observation\n",
        "    )\n",
        "\n",
        "  def reset(self, rng: jp.ndarray) -> State:\n",
        "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
        "    rng, rng1, rng2 = jax.random.split(rng, 3)\n",
        "\n",
        "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
        "    qpos = self.sys.qpos0 + jax.random.uniform(\n",
        "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
        "    )\n",
        "    qvel = jax.random.uniform(\n",
        "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
        "    )\n",
        "\n",
        "    data = self.pipeline_init(qpos, qvel)\n",
        "\n",
        "    obs = self._get_obs(data.data, jp.zeros(self.sys.nu))\n",
        "    reward, done, zero = jp.zeros(3)\n",
        "    metrics = {\n",
        "        'forward_reward': zero,\n",
        "        'reward_linvel': zero,\n",
        "        'reward_quadctrl': zero,\n",
        "        'reward_alive': zero,\n",
        "        'x_position': zero,\n",
        "        'y_position': zero,\n",
        "        'distance_from_origin': zero,\n",
        "        'x_velocity': zero,\n",
        "        'y_velocity': zero,\n",
        "    }\n",
        "    return State(data, obs, reward, done, metrics)\n",
        "\n",
        "  def step(self, state: State, action: jp.ndarray) -> State:\n",
        "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
        "    data0 = state.pipeline_state\n",
        "    data = self.pipeline_step(data0, action)\n",
        "\n",
        "    com_before = data0.data.subtree_com[1]\n",
        "    com_after = data.data.subtree_com[1]\n",
        "    velocity = (com_after - com_before) / self.dt\n",
        "    forward_reward = self._forward_reward_weight * velocity[0]\n",
        "\n",
        "    min_z, max_z = self._healthy_z_range\n",
        "    is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
        "    is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
        "    if self._terminate_when_unhealthy:\n",
        "      healthy_reward = self._healthy_reward\n",
        "    else:\n",
        "      healthy_reward = self._healthy_reward * is_healthy\n",
        "\n",
        "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
        "\n",
        "    obs = self._get_obs(data.data, action)\n",
        "    reward = forward_reward + healthy_reward - ctrl_cost\n",
        "    done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
        "    state.metrics.update(\n",
        "        forward_reward=forward_reward,\n",
        "        reward_linvel=forward_reward,\n",
        "        reward_quadctrl=-ctrl_cost,\n",
        "        reward_alive=healthy_reward,\n",
        "        x_position=com_after[0],\n",
        "        y_position=com_after[1],\n",
        "        distance_from_origin=jp.linalg.norm(com_after),\n",
        "        x_velocity=velocity[0],\n",
        "        y_velocity=velocity[1],\n",
        "    )\n",
        "\n",
        "    return state.replace(\n",
        "        pipeline_state=data, obs=obs, reward=reward, done=done\n",
        "    )\n",
        "\n",
        "  def _get_obs(\n",
        "      self, data: mjx.Data, action: jp.ndarray\n",
        "  ) -> jp.ndarray:\n",
        "    \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
        "    position = data.qpos\n",
        "    if self._exclude_current_positions_from_observation:\n",
        "      position = position[2:]\n",
        "\n",
        "    # external_contact_forces are excluded\n",
        "    return jp.concatenate([\n",
        "        position,\n",
        "        data.qvel,\n",
        "        data.cinert[1:].ravel(),\n",
        "        data.cvel[1:].ravel(),\n",
        "        data.qfrc_actuator,\n",
        "    ])\n",
        "\n",
        "\n",
        "envs.register_environment('humanoid', Humanoid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1K6IznI2y83"
      },
      "source": [
        "## Visualize a Rollout\n",
        "\n",
        "Let's instantiate the environment and visualize a short rollout.\n",
        "\n",
        "NOTE: Since episodes terminates early if the torso is below the healthy z-range, the only relevant contacts for this task are between the feet and the plane. We turn off other contacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EhKLFK54C1CH"
      },
      "outputs": [],
      "source": [
        "# instantiate the environment\n",
        "env_name = 'humanoid'\n",
        "env = envs.get_environment(env_name)\n",
        "\n",
        "# define the jit reset/step functions\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQDG6NQ1CbZD"
      },
      "source": [
        "## Train Humanoid Policy\n",
        "\n",
        "Let's now train a policy with PPO to make the Humanoid run forwards. Training takes about 9-10 minutes on a Tesla A100 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuy004\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/talmolab/Desktop/Salk-Research/Brax-Rodent-Run/wandb/run-20240202_155222-ktstok2e</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/yuy004/vnl_debug/runs/ktstok2e' target=\"_blank\">rosy-lake-41</a></strong> to <a href='https://wandb.ai/yuy004/vnl_debug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/yuy004/vnl_debug' target=\"_blank\">https://wandb.ai/yuy004/vnl_debug</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/yuy004/vnl_debug/runs/ktstok2e' target=\"_blank\">https://wandb.ai/yuy004/vnl_debug/runs/ktstok2e</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"vnl_debug\"\n",
        ")\n",
        "\n",
        "\n",
        "wandb.run.name = f\"humanoid_brax\"\n",
        "\n",
        "def wandb_progress(num_steps, metrics):\n",
        "    print(metrics)\n",
        "    metrics[\"num_steps\"] = num_steps\n",
        "    wandb.log(metrics)\n",
        "    print(metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xLiddQYPApBw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval/walltime': 33.08391284942627, 'eval/episode_distance_from_origin': Array(14.28229, dtype=float32), 'eval/episode_forward_reward': Array(-0.25283659, dtype=float32), 'eval/episode_reward': Array(66.423355, dtype=float32), 'eval/episode_reward_alive': Array(86.44531, dtype=float32), 'eval/episode_reward_linvel': Array(-0.25283659, dtype=float32), 'eval/episode_reward_quadctrl': Array(-19.76912, dtype=float32), 'eval/episode_x_position': Array(0.22101043, dtype=float32), 'eval/episode_x_velocity': Array(-0.2022693, dtype=float32), 'eval/episode_y_position': Array(-0.07978757, dtype=float32), 'eval/episode_y_velocity': Array(-0.58128655, dtype=float32), 'eval/episode_distance_from_origin_std': Array(2.5916872, dtype=float32), 'eval/episode_forward_reward_std': Array(3.784873, dtype=float32), 'eval/episode_reward_std': Array(13.045336, dtype=float32), 'eval/episode_reward_alive_std': Array(14.936746, dtype=float32), 'eval/episode_reward_linvel_std': Array(3.784873, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(3.7181091, dtype=float32), 'eval/episode_x_position_std': Array(0.5888812, dtype=float32), 'eval/episode_x_velocity_std': Array(3.0278983, dtype=float32), 'eval/episode_y_position_std': Array(0.543524, dtype=float32), 'eval/episode_y_velocity_std': Array(3.323149, dtype=float32), 'eval/avg_episode_length': Array(17.289062, dtype=float32), 'eval/epoch_eval_time': 33.08391284942627, 'eval/sps': 1934.4749301958661}\n",
            "{'eval/walltime': 33.08391284942627, 'eval/episode_distance_from_origin': Array(14.28229, dtype=float32), 'eval/episode_forward_reward': Array(-0.25283659, dtype=float32), 'eval/episode_reward': Array(66.423355, dtype=float32), 'eval/episode_reward_alive': Array(86.44531, dtype=float32), 'eval/episode_reward_linvel': Array(-0.25283659, dtype=float32), 'eval/episode_reward_quadctrl': Array(-19.76912, dtype=float32), 'eval/episode_x_position': Array(0.22101043, dtype=float32), 'eval/episode_x_velocity': Array(-0.2022693, dtype=float32), 'eval/episode_y_position': Array(-0.07978757, dtype=float32), 'eval/episode_y_velocity': Array(-0.58128655, dtype=float32), 'eval/episode_distance_from_origin_std': Array(2.5916872, dtype=float32), 'eval/episode_forward_reward_std': Array(3.784873, dtype=float32), 'eval/episode_reward_std': Array(13.045336, dtype=float32), 'eval/episode_reward_alive_std': Array(14.936746, dtype=float32), 'eval/episode_reward_linvel_std': Array(3.784873, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(3.7181091, dtype=float32), 'eval/episode_x_position_std': Array(0.5888812, dtype=float32), 'eval/episode_x_velocity_std': Array(3.0278983, dtype=float32), 'eval/episode_y_position_std': Array(0.543524, dtype=float32), 'eval/episode_y_velocity_std': Array(3.323149, dtype=float32), 'eval/avg_episode_length': Array(17.289062, dtype=float32), 'eval/epoch_eval_time': 33.08391284942627, 'eval/sps': 1934.4749301958661, 'num_steps': 0}\n",
            "{'eval/walltime': 39.45797157287598, 'training/sps': 6325.0101906428, 'training/walltime': 25.903515577316284, 'training/entropy_loss': Array(-0.01341071, dtype=float32), 'training/policy_loss': Array(0.23926774, dtype=float32), 'training/total_loss': Array(0.3188493, dtype=float32), 'training/v_loss': Array(0.09299227, dtype=float32), 'eval/episode_distance_from_origin': Array(14.660873, dtype=float32), 'eval/episode_forward_reward': Array(2.1614876, dtype=float32), 'eval/episode_reward': Array(83.262054, dtype=float32), 'eval/episode_reward_alive': Array(91.28906, dtype=float32), 'eval/episode_reward_linvel': Array(2.1614876, dtype=float32), 'eval/episode_reward_quadctrl': Array(-10.188486, dtype=float32), 'eval/episode_x_position': Array(0.5846597, dtype=float32), 'eval/episode_x_velocity': Array(1.7291901, dtype=float32), 'eval/episode_y_position': Array(0.08936084, dtype=float32), 'eval/episode_y_velocity': Array(0.4375768, dtype=float32), 'eval/episode_distance_from_origin_std': Array(4.5930114, dtype=float32), 'eval/episode_forward_reward_std': Array(2.6423745, dtype=float32), 'eval/episode_reward_std': Array(25.073742, dtype=float32), 'eval/episode_reward_alive_std': Array(27.459103, dtype=float32), 'eval/episode_reward_linvel_std': Array(2.6423745, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(3.2058659, dtype=float32), 'eval/episode_x_position_std': Array(0.46913952, dtype=float32), 'eval/episode_x_velocity_std': Array(2.1138995, dtype=float32), 'eval/episode_y_position_std': Array(0.41033298, dtype=float32), 'eval/episode_y_velocity_std': Array(2.1938174, dtype=float32), 'eval/avg_episode_length': Array(18.257812, dtype=float32), 'eval/epoch_eval_time': 6.374058723449707, 'eval/sps': 10040.698207650421}\n",
            "{'eval/walltime': 39.45797157287598, 'training/sps': 6325.0101906428, 'training/walltime': 25.903515577316284, 'training/entropy_loss': Array(-0.01341071, dtype=float32), 'training/policy_loss': Array(0.23926774, dtype=float32), 'training/total_loss': Array(0.3188493, dtype=float32), 'training/v_loss': Array(0.09299227, dtype=float32), 'eval/episode_distance_from_origin': Array(14.660873, dtype=float32), 'eval/episode_forward_reward': Array(2.1614876, dtype=float32), 'eval/episode_reward': Array(83.262054, dtype=float32), 'eval/episode_reward_alive': Array(91.28906, dtype=float32), 'eval/episode_reward_linvel': Array(2.1614876, dtype=float32), 'eval/episode_reward_quadctrl': Array(-10.188486, dtype=float32), 'eval/episode_x_position': Array(0.5846597, dtype=float32), 'eval/episode_x_velocity': Array(1.7291901, dtype=float32), 'eval/episode_y_position': Array(0.08936084, dtype=float32), 'eval/episode_y_velocity': Array(0.4375768, dtype=float32), 'eval/episode_distance_from_origin_std': Array(4.5930114, dtype=float32), 'eval/episode_forward_reward_std': Array(2.6423745, dtype=float32), 'eval/episode_reward_std': Array(25.073742, dtype=float32), 'eval/episode_reward_alive_std': Array(27.459103, dtype=float32), 'eval/episode_reward_linvel_std': Array(2.6423745, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(3.2058659, dtype=float32), 'eval/episode_x_position_std': Array(0.46913952, dtype=float32), 'eval/episode_x_velocity_std': Array(2.1138995, dtype=float32), 'eval/episode_y_position_std': Array(0.41033298, dtype=float32), 'eval/episode_y_velocity_std': Array(2.1938174, dtype=float32), 'eval/avg_episode_length': Array(18.257812, dtype=float32), 'eval/epoch_eval_time': 6.374058723449707, 'eval/sps': 10040.698207650421, 'num_steps': 163840}\n",
            "{'eval/walltime': 45.846349000930786, 'training/sps': 44336.80247539569, 'training/walltime': 29.598865509033203, 'training/entropy_loss': Array(-0.01324464, dtype=float32), 'training/policy_loss': Array(-0.05817721, dtype=float32), 'training/total_loss': Array(0.07555677, dtype=float32), 'training/v_loss': Array(0.14697862, dtype=float32), 'eval/episode_distance_from_origin': Array(19.401575, dtype=float32), 'eval/episode_forward_reward': Array(2.2988276, dtype=float32), 'eval/episode_reward': Array(109.440216, dtype=float32), 'eval/episode_reward_alive': Array(120.78125, dtype=float32), 'eval/episode_reward_linvel': Array(2.2988276, dtype=float32), 'eval/episode_reward_quadctrl': Array(-13.639861, dtype=float32), 'eval/episode_x_position': Array(0.7926097, dtype=float32), 'eval/episode_x_velocity': Array(1.8390622, dtype=float32), 'eval/episode_y_position': Array(0.09366725, dtype=float32), 'eval/episode_y_velocity': Array(0.604758, dtype=float32), 'eval/episode_distance_from_origin_std': Array(5.980379, dtype=float32), 'eval/episode_forward_reward_std': Array(4.439127, dtype=float32), 'eval/episode_reward_std': Array(32.77057, dtype=float32), 'eval/episode_reward_alive_std': Array(36.18765, dtype=float32), 'eval/episode_reward_linvel_std': Array(4.439127, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.101679, dtype=float32), 'eval/episode_x_position_std': Array(0.8887261, dtype=float32), 'eval/episode_x_velocity_std': Array(3.551302, dtype=float32), 'eval/episode_y_position_std': Array(1.3165556, dtype=float32), 'eval/episode_y_velocity_std': Array(4.5634694, dtype=float32), 'eval/avg_episode_length': Array(24.15625, dtype=float32), 'eval/epoch_eval_time': 6.38837742805481, 'eval/sps': 10018.193308200842}\n",
            "{'eval/walltime': 45.846349000930786, 'training/sps': 44336.80247539569, 'training/walltime': 29.598865509033203, 'training/entropy_loss': Array(-0.01324464, dtype=float32), 'training/policy_loss': Array(-0.05817721, dtype=float32), 'training/total_loss': Array(0.07555677, dtype=float32), 'training/v_loss': Array(0.14697862, dtype=float32), 'eval/episode_distance_from_origin': Array(19.401575, dtype=float32), 'eval/episode_forward_reward': Array(2.2988276, dtype=float32), 'eval/episode_reward': Array(109.440216, dtype=float32), 'eval/episode_reward_alive': Array(120.78125, dtype=float32), 'eval/episode_reward_linvel': Array(2.2988276, dtype=float32), 'eval/episode_reward_quadctrl': Array(-13.639861, dtype=float32), 'eval/episode_x_position': Array(0.7926097, dtype=float32), 'eval/episode_x_velocity': Array(1.8390622, dtype=float32), 'eval/episode_y_position': Array(0.09366725, dtype=float32), 'eval/episode_y_velocity': Array(0.604758, dtype=float32), 'eval/episode_distance_from_origin_std': Array(5.980379, dtype=float32), 'eval/episode_forward_reward_std': Array(4.439127, dtype=float32), 'eval/episode_reward_std': Array(32.77057, dtype=float32), 'eval/episode_reward_alive_std': Array(36.18765, dtype=float32), 'eval/episode_reward_linvel_std': Array(4.439127, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.101679, dtype=float32), 'eval/episode_x_position_std': Array(0.8887261, dtype=float32), 'eval/episode_x_velocity_std': Array(3.551302, dtype=float32), 'eval/episode_y_position_std': Array(1.3165556, dtype=float32), 'eval/episode_y_velocity_std': Array(4.5634694, dtype=float32), 'eval/avg_episode_length': Array(24.15625, dtype=float32), 'eval/epoch_eval_time': 6.38837742805481, 'eval/sps': 10018.193308200842, 'num_steps': 327680}\n",
            "{'eval/walltime': 52.28113651275635, 'training/sps': 43810.745786721905, 'training/walltime': 33.338587284088135, 'training/entropy_loss': Array(-0.013023, dtype=float32), 'training/policy_loss': Array(-0.05840547, dtype=float32), 'training/total_loss': Array(0.09527679, dtype=float32), 'training/v_loss': Array(0.16670528, dtype=float32), 'eval/episode_distance_from_origin': Array(25.064034, dtype=float32), 'eval/episode_forward_reward': Array(4.289277, dtype=float32), 'eval/episode_reward': Array(141.86574, dtype=float32), 'eval/episode_reward_alive': Array(155.11719, dtype=float32), 'eval/episode_reward_linvel': Array(4.289277, dtype=float32), 'eval/episode_reward_quadctrl': Array(-17.540722, dtype=float32), 'eval/episode_x_position': Array(1.4036732, dtype=float32), 'eval/episode_x_velocity': Array(3.431422, dtype=float32), 'eval/episode_y_position': Array(0.74471545, dtype=float32), 'eval/episode_y_velocity': Array(2.6389637, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6.576399, dtype=float32), 'eval/episode_forward_reward_std': Array(6.9219284, dtype=float32), 'eval/episode_reward_std': Array(36.169075, dtype=float32), 'eval/episode_reward_alive_std': Array(39.570274, dtype=float32), 'eval/episode_reward_linvel_std': Array(6.9219284, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.6151094, dtype=float32), 'eval/episode_x_position_std': Array(1.6523467, dtype=float32), 'eval/episode_x_velocity_std': Array(5.537543, dtype=float32), 'eval/episode_y_position_std': Array(1.6019174, dtype=float32), 'eval/episode_y_velocity_std': Array(5.7322164, dtype=float32), 'eval/avg_episode_length': Array(31.023438, dtype=float32), 'eval/epoch_eval_time': 6.4347875118255615, 'eval/sps': 9945.938367410532}\n",
            "{'eval/walltime': 52.28113651275635, 'training/sps': 43810.745786721905, 'training/walltime': 33.338587284088135, 'training/entropy_loss': Array(-0.013023, dtype=float32), 'training/policy_loss': Array(-0.05840547, dtype=float32), 'training/total_loss': Array(0.09527679, dtype=float32), 'training/v_loss': Array(0.16670528, dtype=float32), 'eval/episode_distance_from_origin': Array(25.064034, dtype=float32), 'eval/episode_forward_reward': Array(4.289277, dtype=float32), 'eval/episode_reward': Array(141.86574, dtype=float32), 'eval/episode_reward_alive': Array(155.11719, dtype=float32), 'eval/episode_reward_linvel': Array(4.289277, dtype=float32), 'eval/episode_reward_quadctrl': Array(-17.540722, dtype=float32), 'eval/episode_x_position': Array(1.4036732, dtype=float32), 'eval/episode_x_velocity': Array(3.431422, dtype=float32), 'eval/episode_y_position': Array(0.74471545, dtype=float32), 'eval/episode_y_velocity': Array(2.6389637, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6.576399, dtype=float32), 'eval/episode_forward_reward_std': Array(6.9219284, dtype=float32), 'eval/episode_reward_std': Array(36.169075, dtype=float32), 'eval/episode_reward_alive_std': Array(39.570274, dtype=float32), 'eval/episode_reward_linvel_std': Array(6.9219284, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.6151094, dtype=float32), 'eval/episode_x_position_std': Array(1.6523467, dtype=float32), 'eval/episode_x_velocity_std': Array(5.537543, dtype=float32), 'eval/episode_y_position_std': Array(1.6019174, dtype=float32), 'eval/episode_y_velocity_std': Array(5.7322164, dtype=float32), 'eval/avg_episode_length': Array(31.023438, dtype=float32), 'eval/epoch_eval_time': 6.4347875118255615, 'eval/sps': 9945.938367410532, 'num_steps': 491520}\n",
            "{'eval/walltime': 58.7310426235199, 'training/sps': 43959.77517909331, 'training/walltime': 37.06563091278076, 'training/entropy_loss': Array(-0.0128231, dtype=float32), 'training/policy_loss': Array(-0.0459944, dtype=float32), 'training/total_loss': Array(0.09050432, dtype=float32), 'training/v_loss': Array(0.14932182, dtype=float32), 'eval/episode_distance_from_origin': Array(29.040007, dtype=float32), 'eval/episode_forward_reward': Array(5.82216, dtype=float32), 'eval/episode_reward': Array(164.7081, dtype=float32), 'eval/episode_reward_alive': Array(179.0625, dtype=float32), 'eval/episode_reward_linvel': Array(5.82216, dtype=float32), 'eval/episode_reward_quadctrl': Array(-20.176567, dtype=float32), 'eval/episode_x_position': Array(1.9401579, dtype=float32), 'eval/episode_x_velocity': Array(4.657728, dtype=float32), 'eval/episode_y_position': Array(0.5582854, dtype=float32), 'eval/episode_y_velocity': Array(2.7042708, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6.8017797, dtype=float32), 'eval/episode_forward_reward_std': Array(8.636925, dtype=float32), 'eval/episode_reward_std': Array(37.780872, dtype=float32), 'eval/episode_reward_alive_std': Array(40.41653, dtype=float32), 'eval/episode_reward_linvel_std': Array(8.636925, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.656371, dtype=float32), 'eval/episode_x_position_std': Array(2.0507798, dtype=float32), 'eval/episode_x_velocity_std': Array(6.9095397, dtype=float32), 'eval/episode_y_position_std': Array(2.438067, dtype=float32), 'eval/episode_y_velocity_std': Array(7.2697473, dtype=float32), 'eval/avg_episode_length': Array(35.8125, dtype=float32), 'eval/epoch_eval_time': 6.44990611076355, 'eval/sps': 9922.62505855664}\n",
            "{'eval/walltime': 58.7310426235199, 'training/sps': 43959.77517909331, 'training/walltime': 37.06563091278076, 'training/entropy_loss': Array(-0.0128231, dtype=float32), 'training/policy_loss': Array(-0.0459944, dtype=float32), 'training/total_loss': Array(0.09050432, dtype=float32), 'training/v_loss': Array(0.14932182, dtype=float32), 'eval/episode_distance_from_origin': Array(29.040007, dtype=float32), 'eval/episode_forward_reward': Array(5.82216, dtype=float32), 'eval/episode_reward': Array(164.7081, dtype=float32), 'eval/episode_reward_alive': Array(179.0625, dtype=float32), 'eval/episode_reward_linvel': Array(5.82216, dtype=float32), 'eval/episode_reward_quadctrl': Array(-20.176567, dtype=float32), 'eval/episode_x_position': Array(1.9401579, dtype=float32), 'eval/episode_x_velocity': Array(4.657728, dtype=float32), 'eval/episode_y_position': Array(0.5582854, dtype=float32), 'eval/episode_y_velocity': Array(2.7042708, dtype=float32), 'eval/episode_distance_from_origin_std': Array(6.8017797, dtype=float32), 'eval/episode_forward_reward_std': Array(8.636925, dtype=float32), 'eval/episode_reward_std': Array(37.780872, dtype=float32), 'eval/episode_reward_alive_std': Array(40.41653, dtype=float32), 'eval/episode_reward_linvel_std': Array(8.636925, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(4.656371, dtype=float32), 'eval/episode_x_position_std': Array(2.0507798, dtype=float32), 'eval/episode_x_velocity_std': Array(6.9095397, dtype=float32), 'eval/episode_y_position_std': Array(2.438067, dtype=float32), 'eval/episode_y_velocity_std': Array(7.2697473, dtype=float32), 'eval/avg_episode_length': Array(35.8125, dtype=float32), 'eval/epoch_eval_time': 6.44990611076355, 'eval/sps': 9922.62505855664, 'num_steps': 655360}\n",
            "{'eval/walltime': 65.2302188873291, 'training/sps': 43769.87110337221, 'training/walltime': 40.80884504318237, 'training/entropy_loss': Array(-0.01263842, dtype=float32), 'training/policy_loss': Array(-0.03301621, dtype=float32), 'training/total_loss': Array(0.06609072, dtype=float32), 'training/v_loss': Array(0.11174534, dtype=float32), 'eval/episode_distance_from_origin': Array(32.440327, dtype=float32), 'eval/episode_forward_reward': Array(7.5796885, dtype=float32), 'eval/episode_reward': Array(184.01584, dtype=float32), 'eval/episode_reward_alive': Array(198.82812, dtype=float32), 'eval/episode_reward_linvel': Array(7.5796885, dtype=float32), 'eval/episode_reward_quadctrl': Array(-22.39197, dtype=float32), 'eval/episode_x_position': Array(2.6949637, dtype=float32), 'eval/episode_x_velocity': Array(6.063751, dtype=float32), 'eval/episode_y_position': Array(0.17208277, dtype=float32), 'eval/episode_y_velocity': Array(1.1665876, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.64251, dtype=float32), 'eval/episode_forward_reward_std': Array(9.488989, dtype=float32), 'eval/episode_reward_std': Array(46.943336, dtype=float32), 'eval/episode_reward_alive_std': Array(50.185143, dtype=float32), 'eval/episode_reward_linvel_std': Array(9.488989, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.704006, dtype=float32), 'eval/episode_x_position_std': Array(2.7638211, dtype=float32), 'eval/episode_x_velocity_std': Array(7.591191, dtype=float32), 'eval/episode_y_position_std': Array(3.4531498, dtype=float32), 'eval/episode_y_velocity_std': Array(8.861995, dtype=float32), 'eval/avg_episode_length': Array(39.765625, dtype=float32), 'eval/epoch_eval_time': 6.499176263809204, 'eval/sps': 9847.401794037394}\n",
            "{'eval/walltime': 65.2302188873291, 'training/sps': 43769.87110337221, 'training/walltime': 40.80884504318237, 'training/entropy_loss': Array(-0.01263842, dtype=float32), 'training/policy_loss': Array(-0.03301621, dtype=float32), 'training/total_loss': Array(0.06609072, dtype=float32), 'training/v_loss': Array(0.11174534, dtype=float32), 'eval/episode_distance_from_origin': Array(32.440327, dtype=float32), 'eval/episode_forward_reward': Array(7.5796885, dtype=float32), 'eval/episode_reward': Array(184.01584, dtype=float32), 'eval/episode_reward_alive': Array(198.82812, dtype=float32), 'eval/episode_reward_linvel': Array(7.5796885, dtype=float32), 'eval/episode_reward_quadctrl': Array(-22.39197, dtype=float32), 'eval/episode_x_position': Array(2.6949637, dtype=float32), 'eval/episode_x_velocity': Array(6.063751, dtype=float32), 'eval/episode_y_position': Array(0.17208277, dtype=float32), 'eval/episode_y_velocity': Array(1.1665876, dtype=float32), 'eval/episode_distance_from_origin_std': Array(8.64251, dtype=float32), 'eval/episode_forward_reward_std': Array(9.488989, dtype=float32), 'eval/episode_reward_std': Array(46.943336, dtype=float32), 'eval/episode_reward_alive_std': Array(50.185143, dtype=float32), 'eval/episode_reward_linvel_std': Array(9.488989, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.704006, dtype=float32), 'eval/episode_x_position_std': Array(2.7638211, dtype=float32), 'eval/episode_x_velocity_std': Array(7.591191, dtype=float32), 'eval/episode_y_position_std': Array(3.4531498, dtype=float32), 'eval/episode_y_velocity_std': Array(8.861995, dtype=float32), 'eval/avg_episode_length': Array(39.765625, dtype=float32), 'eval/epoch_eval_time': 6.499176263809204, 'eval/sps': 9847.401794037394, 'num_steps': 819200}\n",
            "{'eval/walltime': 71.69503903388977, 'training/sps': 43384.81999297832, 'training/walltime': 44.58528113365173, 'training/entropy_loss': Array(-0.01248693, dtype=float32), 'training/policy_loss': Array(-0.02427561, dtype=float32), 'training/total_loss': Array(0.0566736, dtype=float32), 'training/v_loss': Array(0.09343614, dtype=float32), 'eval/episode_distance_from_origin': Array(32.819305, dtype=float32), 'eval/episode_forward_reward': Array(9.717481, dtype=float32), 'eval/episode_reward': Array(188.54005, dtype=float32), 'eval/episode_reward_alive': Array(201.32812, dtype=float32), 'eval/episode_reward_linvel': Array(9.717481, dtype=float32), 'eval/episode_reward_quadctrl': Array(-22.505543, dtype=float32), 'eval/episode_x_position': Array(3.4467697, dtype=float32), 'eval/episode_x_velocity': Array(7.773985, dtype=float32), 'eval/episode_y_position': Array(0.3769203, dtype=float32), 'eval/episode_y_velocity': Array(0.95813644, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7.551639, dtype=float32), 'eval/episode_forward_reward_std': Array(8.879068, dtype=float32), 'eval/episode_reward_std': Array(42.12623, dtype=float32), 'eval/episode_reward_alive_std': Array(45.622787, dtype=float32), 'eval/episode_reward_linvel_std': Array(8.879068, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.348186, dtype=float32), 'eval/episode_x_position_std': Array(2.300839, dtype=float32), 'eval/episode_x_velocity_std': Array(7.1032553, dtype=float32), 'eval/episode_y_position_std': Array(2.822265, dtype=float32), 'eval/episode_y_velocity_std': Array(8.613825, dtype=float32), 'eval/avg_episode_length': Array(40.265625, dtype=float32), 'eval/epoch_eval_time': 6.464820146560669, 'eval/sps': 9899.734029576748}\n",
            "{'eval/walltime': 71.69503903388977, 'training/sps': 43384.81999297832, 'training/walltime': 44.58528113365173, 'training/entropy_loss': Array(-0.01248693, dtype=float32), 'training/policy_loss': Array(-0.02427561, dtype=float32), 'training/total_loss': Array(0.0566736, dtype=float32), 'training/v_loss': Array(0.09343614, dtype=float32), 'eval/episode_distance_from_origin': Array(32.819305, dtype=float32), 'eval/episode_forward_reward': Array(9.717481, dtype=float32), 'eval/episode_reward': Array(188.54005, dtype=float32), 'eval/episode_reward_alive': Array(201.32812, dtype=float32), 'eval/episode_reward_linvel': Array(9.717481, dtype=float32), 'eval/episode_reward_quadctrl': Array(-22.505543, dtype=float32), 'eval/episode_x_position': Array(3.4467697, dtype=float32), 'eval/episode_x_velocity': Array(7.773985, dtype=float32), 'eval/episode_y_position': Array(0.3769203, dtype=float32), 'eval/episode_y_velocity': Array(0.95813644, dtype=float32), 'eval/episode_distance_from_origin_std': Array(7.551639, dtype=float32), 'eval/episode_forward_reward_std': Array(8.879068, dtype=float32), 'eval/episode_reward_std': Array(42.12623, dtype=float32), 'eval/episode_reward_alive_std': Array(45.622787, dtype=float32), 'eval/episode_reward_linvel_std': Array(8.879068, dtype=float32), 'eval/episode_reward_quadctrl_std': Array(5.348186, dtype=float32), 'eval/episode_x_position_std': Array(2.300839, dtype=float32), 'eval/episode_x_velocity_std': Array(7.1032553, dtype=float32), 'eval/episode_y_position_std': Array(2.822265, dtype=float32), 'eval/episode_y_velocity_std': Array(8.613825, dtype=float32), 'eval/avg_episode_length': Array(40.265625, dtype=float32), 'eval/epoch_eval_time': 6.464820146560669, 'eval/sps': 9899.734029576748, 'num_steps': 983040}\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m train_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m      2\u001b[0m     ppo\u001b[38;5;241m.\u001b[39mtrain, num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3_000_000\u001b[39m, num_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, reward_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      3\u001b[0m     episode_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, normalize_observations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, action_repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      4\u001b[0m     unroll_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_minibatches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_updates_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m      5\u001b[0m     discounting\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.97\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m, entropy_cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m      6\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m make_inference_fn, params, _\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_progress\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/stac-mjx/lib/python3.12/site-packages/brax/training/agents/ppo/train.py:435\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(environment, num_timesteps, episode_length, action_repeat, num_envs, max_devices_per_host, num_eval_envs, learning_rate, entropy_cost, discounting, seed, unroll_length, batch_size, num_minibatches, num_updates_per_batch, num_evals, num_resets_per_eval, normalize_observations, reward_scaling, clipping_epsilon, gae_lambda, deterministic_eval, network_factory, progress_fn, normalize_advantage, eval_env, policy_params_fn, randomization_fn)\u001b[0m\n\u001b[1;32m    431\u001b[0m   env_state \u001b[38;5;241m=\u001b[39m reset_fn(key_envs) \u001b[38;5;28;01mif\u001b[39;00m num_resets_per_eval \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m env_state\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    434\u001b[0m   \u001b[38;5;66;03m# Run evals.\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m   metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_unpmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m          \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalizer_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtraining_metrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m   logging\u001b[38;5;241m.\u001b[39minfo(metrics)\n\u001b[1;32m    440\u001b[0m   progress_fn(current_step, metrics)\n",
            "File \u001b[0;32m~/anaconda3/envs/stac-mjx/lib/python3.12/site-packages/brax/training/acting.py:125\u001b[0m, in \u001b[0;36mEvaluator.run_evaluation\u001b[0;34m(self, policy_params, training_metrics, aggregate_episodes)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key, unroll_key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key)\n\u001b[1;32m    124\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 125\u001b[0m eval_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_eval_unroll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munroll_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m eval_metrics \u001b[38;5;241m=\u001b[39m eval_state\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_metrics\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    127\u001b[0m eval_metrics\u001b[38;5;241m.\u001b[39mactive_episodes\u001b[38;5;241m.\u001b[39mblock_until_ready()\n",
            "File \u001b[0;32m~/anaconda3/envs/stac-mjx/lib/python3.12/site-packages/flax/struct.py:129\u001b[0m, in \u001b[0;36mdataclass.<locals>.clz_from_iterable\u001b[0;34m(meta, data)\u001b[0m\n\u001b[1;32m    123\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    124\u001b[0m       (jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mGetAttrKey(name), \u001b[38;5;28mgetattr\u001b[39m(x, name))\n\u001b[1;32m    125\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m data_fields\n\u001b[1;32m    126\u001b[0m   )\n\u001b[1;32m    127\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m data, meta\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclz_from_iterable\u001b[39m(meta, data):\n\u001b[1;32m    130\u001b[0m   meta_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(meta_fields, meta))\n\u001b[1;32m    131\u001b[0m   data_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(data_fields, data))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_fn = functools.partial(\n",
        "    ppo.train, num_timesteps=3_000_000, num_evals=1000, reward_scaling=0.1,\n",
        "    episode_length=500, normalize_observations=True, action_repeat=1,\n",
        "    unroll_length=10, num_minibatches=32, num_updates_per_batch=8,\n",
        "    discounting=0.97, learning_rate=3e-4, entropy_cost=1e-3, num_envs=1024,\n",
        "    batch_size=512, seed=0)\n",
        "\n",
        "\n",
        "make_inference_fn, params, _= train_fn(environment=env, progress_fn=wandb_progress)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYIch0HEApBx"
      },
      "source": [
        "<!-- ## Save and Load Policy -->\n",
        "\n",
        "We can save and load the policy using the brax model API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8gI6qH6ApBx"
      },
      "outputs": [],
      "source": [
        "#@title Save Model\n",
        "model_path = '/tmp/mjx_brax_policy'\n",
        "model.save_params(model_path, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4reaWgxApBx"
      },
      "outputs": [],
      "source": [
        "#@title Load Model and Define Inference Function\n",
        "params = model.load_params(model_path)\n",
        "\n",
        "inference_fn = make_inference_fn(params)\n",
        "jit_inference_fn = jax.jit(inference_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G357XIfApBy"
      },
      "source": [
        "## Visualize Policy\n",
        "\n",
        "Finally we can visualize the policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osYasMw4ApBy"
      },
      "outputs": [],
      "source": [
        "eval_env = envs.get_environment(env_name)\n",
        "\n",
        "jit_reset = jax.jit(eval_env.reset)\n",
        "jit_step = jax.jit(eval_env.step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-UhypudApBy"
      },
      "outputs": [],
      "source": [
        "# initialize the state\n",
        "rng = jax.random.PRNGKey(0)\n",
        "state = jit_reset(rng)\n",
        "rollout = [state.pipeline_state]\n",
        "\n",
        "# grab a trajectory\n",
        "n_steps = 500\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps):\n",
        "  act_rng, rng = jax.random.split(rng)\n",
        "  ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
        "  state = jit_step(state, ctrl)\n",
        "  rollout.append(state.pipeline_state)\n",
        "\n",
        "  if state.done:\n",
        "    break\n",
        "\n",
        "media.show_video(env.render(rollout[::render_every], camera='side'), fps=1.0 / env.dt / render_every)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR-heox6LARK"
      },
      "source": [
        "# MJX Policy in MuJoCo\n",
        "\n",
        "We can also perform the physics step using the original MuJoCo python bindings to show that the policy trained in MJX works in MuJoCo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6ixFi4dApBy"
      },
      "outputs": [],
      "source": [
        "mj_model = eval_env._model\n",
        "mj_data = mujoco.MjData(mj_model)\n",
        "\n",
        "renderer = mujoco.Renderer(mj_model)\n",
        "ctrl = jp.zeros(mj_model.nu)\n",
        "\n",
        "images = []\n",
        "for i in range(n_steps):\n",
        "  act_rng, rng = jax.random.split(rng)\n",
        "\n",
        "  obs = eval_env._get_obs(mjx.put_data(mj_model, mj_data), ctrl)\n",
        "  ctrl, _ = jit_inference_fn(obs, act_rng)\n",
        "\n",
        "  mj_data.ctrl = ctrl\n",
        "  for _ in range(eval_env._n_frames):\n",
        "    mujoco.mj_step(mj_model, mj_data)  # Physics step using MuJoCo mj_step.\n",
        "\n",
        "  if i % render_every == 0:\n",
        "    renderer.update_scene(mj_data, camera='side')\n",
        "    images.append(renderer.render())\n",
        "\n",
        "media.show_video(images, fps=1.0 / eval_env.dt / render_every)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65mIPj6DQNNa"
      },
      "source": [
        "# Training a Policy with Domain Randomization\n",
        "\n",
        "We might also want to include randomization over certain `mjModel` parameters while training a policy. In MJX, we can easily create a batch of environments with randomized values populated in `mjx.Model`. Below, we show a function that randomizes friction and actuator gain/bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8mhzKjHQuoL"
      },
      "outputs": [],
      "source": [
        "def domain_randomize(sys, rng):\n",
        "  \"\"\"Randomizes the mjx.Model.\"\"\"\n",
        "  @jax.vmap\n",
        "  def rand(rng):\n",
        "    _, key = jax.random.split(rng, 2)\n",
        "    # friction\n",
        "    friction = jax.random.uniform(key, (1,), minval=0.6, maxval=1.4)\n",
        "    friction = sys.geom_friction.at[:, 0].set(friction)\n",
        "    # actuator\n",
        "    _, key = jax.random.split(key, 2)\n",
        "    gain_range = (-5, 5)\n",
        "    param = jax.random.uniform(\n",
        "        key, (1,), minval=gain_range[0], maxval=gain_range[1]\n",
        "    ) + sys.actuator_gainprm[:, 0]\n",
        "    gain = sys.actuator_gainprm.at[:, 0].set(param)\n",
        "    bias = sys.actuator_biasprm.at[:, 1].set(-param)\n",
        "    return friction, gain, bias\n",
        "\n",
        "  friction, gain, bias = rand(rng)\n",
        "\n",
        "  in_axes = jax.tree_map(lambda x: None, sys)\n",
        "  in_axes = in_axes.tree_replace({\n",
        "      'geom_friction': 0,\n",
        "      'actuator_gainprm': 0,\n",
        "      'actuator_biasprm': 0,\n",
        "  })\n",
        "\n",
        "  sys = sys.tree_replace({\n",
        "      'geom_friction': friction,\n",
        "      'actuator_gainprm': gain,\n",
        "      'actuator_biasprm': bias,\n",
        "  })\n",
        "\n",
        "  return sys, in_axes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnsZo-GWSYYj"
      },
      "source": [
        "If we wanted 10 environments with randomized friction and actuator params, we can call `domain_randomize`, which returns a batched `mjx.Model` along with a dictionary specifying the axes that are batched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K45Kp2ASV9s"
      },
      "outputs": [],
      "source": [
        "rng = jax.random.PRNGKey(0)\n",
        "rng = jax.random.split(rng, 10)\n",
        "batched_sys, _ = domain_randomize(env.sys, rng)\n",
        "\n",
        "print('Single env friction shape: ', env.sys.geom_friction.shape)\n",
        "print('Batched env friction shape: ', batched_sys.geom_friction.shape)\n",
        "\n",
        "print('Friction on geom 0: ', env.sys.geom_friction[0, 0])\n",
        "print('Random frictions on geom 0: ', batched_sys.geom_friction[:, 0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efnxNOnpQFuC"
      },
      "source": [
        "## Quadruped Env\n",
        "\n",
        "Let's define a quadruped environment that takes advantage of the domain randomization function. Here we use the [Barkour vb Quadruped](https://github.com/google-deepmind/mujoco_menagerie/tree/main/google_barkour_vb) from [MuJoCo Menagerie](https://github.com/google-deepmind/mujoco_menagerie). We implement an environment that trains a joystick policy with Brax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfyK73gtRXid"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/google-deepmind/mujoco_menagerie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y79PoJOCIl-O"
      },
      "outputs": [],
      "source": [
        "#@title Barkour vb Quadruped Env\n",
        "\n",
        "def get_config():\n",
        "  \"\"\"Returns reward config for barkour quadruped environment.\"\"\"\n",
        "\n",
        "  def get_default_rewards_config():\n",
        "    default_config = config_dict.ConfigDict(\n",
        "        dict(\n",
        "            # The coefficients for all reward terms used for training. All\n",
        "            # physical quantities are in SI units, if no otherwise specified,\n",
        "            # i.e. joint positions are in rad, positions are measured in meters,\n",
        "            # torques in Nm, and time in seconds, and forces in Newtons.\n",
        "            scales=config_dict.ConfigDict(\n",
        "                dict(\n",
        "                    # Tracking rewards are computed using exp(-delta^2/sigma)\n",
        "                    # sigma can be a hyperparameters to tune.\n",
        "                    # Track the base x-y velocity (no z-velocity tracking.)\n",
        "                    tracking_lin_vel=1.5,\n",
        "                    # Track the angular velocity along z-axis, i.e. yaw rate.\n",
        "                    tracking_ang_vel=0.8,\n",
        "                    # Below are regularization terms, we roughly divide the\n",
        "                    # terms to base state regularizations, joint\n",
        "                    # regularizations, and other behavior regularizations.\n",
        "                    # Penalize the base velocity in z direction, L2 penalty.\n",
        "                    lin_vel_z=-2.0,\n",
        "                    # Penalize the base roll and pitch rate. L2 penalty.\n",
        "                    ang_vel_xy=-0.05,\n",
        "                    # Penalize non-zero roll and pitch angles. L2 penalty.\n",
        "                    orientation=-5.0,\n",
        "                    # L2 regularization of joint torques, |tau|^2.\n",
        "                    torques=-0.0002,\n",
        "                    # Penalize the change in the action and encourage smooth\n",
        "                    # actions. L2 regularization |action - last_action|^2\n",
        "                    action_rate=-0.01,\n",
        "                    # Encourage long swing steps.  However, it does not\n",
        "                    # encourage high clearances.\n",
        "                    feet_air_time=0.2,\n",
        "                    # Encourage no motion at zero command, L2 regularization\n",
        "                    # |q - q_default|^2.\n",
        "                    stand_still=-0.5,\n",
        "                    # Early termination penalty.\n",
        "                    termination=-1.0,\n",
        "                    # Penalizing foot slipping on the ground.\n",
        "                    foot_slip=-0.1,\n",
        "                )\n",
        "            ),\n",
        "            # Tracking reward = exp(-error^2/sigma).\n",
        "            tracking_sigma=0.25,\n",
        "        )\n",
        "    )\n",
        "    return default_config\n",
        "\n",
        "  default_config = config_dict.ConfigDict(\n",
        "      dict(\n",
        "          rewards=get_default_rewards_config(),\n",
        "      )\n",
        "  )\n",
        "\n",
        "  return default_config\n",
        "\n",
        "\n",
        "class BarkourEnv(MjxEnv):\n",
        "  \"\"\"Environment for training the barkour quadruped joystick policy in MJX.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      obs_noise: float = 0.05,\n",
        "      action_scale: float = 0.3,\n",
        "      kick_vel: float = 0.05,\n",
        "      **kwargs,\n",
        "  ):\n",
        "    path = epath.Path('mujoco_menagerie/google_barkour_vb/scene_mjx.xml')\n",
        "    self._dt = 0.02  # this environment is 50 fps\n",
        "    self.brax_sys = mjcf.load(path).replace(dt=self._dt)\n",
        "    model = self.brax_sys.get_model()\n",
        "    model.opt.timestep = 0.004\n",
        "\n",
        "    # override menagerie params for smoother policy\n",
        "    model.dof_damping[6:] = 0.5239\n",
        "    model.actuator_gainprm[:, 0] = 35.0\n",
        "    model.actuator_biasprm[:, 1] = -35.0\n",
        "\n",
        "    n_frames = kwargs.pop('n_frames', int(self._dt / model.opt.timestep))\n",
        "    super().__init__(model=model, n_frames=n_frames)\n",
        "\n",
        "    self.reward_config = get_config()\n",
        "    # set custom from kwargs\n",
        "    for k, v in kwargs.items():\n",
        "      if k.endswith('_scale'):\n",
        "        self.reward_config.rewards.scales[k[:-6]] = v\n",
        "\n",
        "    self._torso_idx = mujoco.mj_name2id(\n",
        "        model, mujoco.mjtObj.mjOBJ_BODY.value, 'torso'\n",
        "    )\n",
        "    self._action_scale = action_scale\n",
        "    self._obs_noise = obs_noise\n",
        "    self._kick_vel = kick_vel\n",
        "    self._init_q = jp.array(model.keyframe('home').qpos)\n",
        "    self._default_pose = model.keyframe('home').qpos[7:]\n",
        "    self.lowers = jp.array([-0.7, -1.0, 0.05] * 4)\n",
        "    self.uppers = jp.array([0.52, 2.1, 2.1] * 4)\n",
        "    feet_site = [\n",
        "        'foot_front_left',\n",
        "        'foot_hind_left',\n",
        "        'foot_front_right',\n",
        "        'foot_hind_right',\n",
        "    ]\n",
        "    feet_site_id = [\n",
        "        mujoco.mj_name2id(model, mujoco.mjtObj.mjOBJ_SITE.value, f)\n",
        "        for f in feet_site\n",
        "    ]\n",
        "    assert not any(id_ == -1 for id_ in feet_site_id), 'Site not found.'\n",
        "    self._feet_site_id = np.array(feet_site_id)\n",
        "    lower_leg_body = [\n",
        "        'lower_leg_front_left',\n",
        "        'lower_leg_hind_left',\n",
        "        'lower_leg_front_right',\n",
        "        'lower_leg_hind_right',\n",
        "    ]\n",
        "    lower_leg_body_id = [\n",
        "        mujoco.mj_name2id(model, mujoco.mjtObj.mjOBJ_BODY.value, l)\n",
        "        for l in lower_leg_body\n",
        "    ]\n",
        "    assert not any(id_ == -1 for id_ in lower_leg_body_id), 'Body not found.'\n",
        "    self._lower_leg_body_id = np.array(lower_leg_body_id)\n",
        "    self._foot_radius = 0.0175\n",
        "    self._nv = model.nv\n",
        "\n",
        "  def sample_command(self, rng: jax.Array) -> jax.Array:\n",
        "    lin_vel_x = [-0.6, 1.5]  # min max [m/s]\n",
        "    lin_vel_y = [-0.8, 0.8]  # min max [m/s]\n",
        "    ang_vel_yaw = [-0.7, 0.7]  # min max [rad/s]\n",
        "\n",
        "    _, key1, key2, key3 = jax.random.split(rng, 4)\n",
        "    lin_vel_x = jax.random.uniform(\n",
        "        key1, (1,), minval=lin_vel_x[0], maxval=lin_vel_x[1]\n",
        "    )\n",
        "    lin_vel_y = jax.random.uniform(\n",
        "        key2, (1,), minval=lin_vel_y[0], maxval=lin_vel_y[1]\n",
        "    )\n",
        "    ang_vel_yaw = jax.random.uniform(\n",
        "        key3, (1,), minval=ang_vel_yaw[0], maxval=ang_vel_yaw[1]\n",
        "    )\n",
        "    new_cmd = jp.array([lin_vel_x[0], lin_vel_y[0], ang_vel_yaw[0]])\n",
        "    return new_cmd\n",
        "\n",
        "  def reset(self, rng: jax.Array) -> State:  # pytype: disable=signature-mismatch\n",
        "    rng, key = jax.random.split(rng)\n",
        "\n",
        "    pipeline_state = self.pipeline_init(self._init_q, jp.zeros(self._nv))\n",
        "\n",
        "    state_info = {\n",
        "        'rng': rng,\n",
        "        'last_act': jp.zeros(12),\n",
        "        'last_vel': jp.zeros(12),\n",
        "        'command': self.sample_command(key),\n",
        "        'last_contact': jp.zeros(4, dtype=bool),\n",
        "        'feet_air_time': jp.zeros(4),\n",
        "        'rewards': {k: 0.0 for k in self.reward_config.rewards.scales.keys()},\n",
        "        'kick': jp.array([0.0, 0.0]),\n",
        "        'step': 0,\n",
        "    }\n",
        "\n",
        "    obs_history = jp.zeros(15 * 31)  # store 15 steps of history\n",
        "    obs = self._get_obs(pipeline_state, state_info, obs_history)\n",
        "    reward, done = jp.zeros(2)\n",
        "    metrics = {'total_dist': 0.0}\n",
        "    for k in state_info['rewards']:\n",
        "      metrics[k] = state_info['rewards'][k]\n",
        "    state = State(pipeline_state, obs, reward, done, metrics, state_info)  # pytype: disable=wrong-arg-types\n",
        "    return state\n",
        "\n",
        "  def step(self, state: State, action: jax.Array) -> State:  # pytype: disable=signature-mismatch\n",
        "    rng, cmd_rng, kick_noise_2 = jax.random.split(state.info['rng'], 3)\n",
        "\n",
        "    # kick\n",
        "    push_interval = 10\n",
        "    kick_theta = jax.random.uniform(kick_noise_2, maxval=2 * jp.pi)\n",
        "    kick = jp.array([jp.cos(kick_theta), jp.sin(kick_theta)])\n",
        "    kick *= jp.mod(state.info['step'], push_interval) == 0\n",
        "    qvel = state.pipeline_state.data.qvel  # pytype: disable=attribute-error\n",
        "    qvel = qvel.at[:2].set(kick * self._kick_vel + qvel[:2])\n",
        "    state = state.tree_replace({'pipeline_state.data.qvel': qvel})\n",
        "\n",
        "    # physics step\n",
        "    motor_targets = self._default_pose + action * self._action_scale\n",
        "    motor_targets = jp.clip(motor_targets, self.lowers, self.uppers)\n",
        "    pipeline_state = self.pipeline_step(state.pipeline_state, motor_targets)\n",
        "    x, xd = pipeline_state.x, pipeline_state.xd\n",
        "\n",
        "    # observation data\n",
        "    obs = self._get_obs(pipeline_state, state.info, state.obs)\n",
        "    joint_angles = pipeline_state.q[7:]\n",
        "    joint_vel = pipeline_state.qd[6:]\n",
        "\n",
        "    # foot contact data based on z-position\n",
        "    foot_pos = pipeline_state.data.site_xpos[self._feet_site_id]  # pytype: disable=attribute-error\n",
        "    foot_contact_z = foot_pos[:, 2] - self._foot_radius\n",
        "    contact = foot_contact_z < 1e-3  # a mm or less off the floor\n",
        "    contact_filt_mm = contact | state.info['last_contact']\n",
        "    contact_filt_cm = (foot_contact_z < 3e-2) | state.info['last_contact']\n",
        "    first_contact = (state.info['feet_air_time'] > 0) * contact_filt_mm\n",
        "    state.info['feet_air_time'] += self.dt\n",
        "\n",
        "    # done if joint limits are reached or robot is falling\n",
        "    up = jp.array([0.0, 0.0, 1.0])\n",
        "    done = jp.dot(math.rotate(up, x.rot[self._torso_idx - 1]), up) < 0\n",
        "    done |= jp.any(joint_angles < self.lowers)\n",
        "    done |= jp.any(joint_angles > self.uppers)\n",
        "    done |= pipeline_state.x.pos[self._torso_idx - 1, 2] < 0.18\n",
        "\n",
        "    # reward\n",
        "    rewards = {\n",
        "        'tracking_lin_vel': (\n",
        "            self._reward_tracking_lin_vel(state.info['command'], x, xd)\n",
        "        ),\n",
        "        'tracking_ang_vel': (\n",
        "            self._reward_tracking_ang_vel(state.info['command'], x, xd)\n",
        "        ),\n",
        "        'lin_vel_z': self._reward_lin_vel_z(xd),\n",
        "        'ang_vel_xy': self._reward_ang_vel_xy(xd),\n",
        "        'orientation': self._reward_orientation(x),\n",
        "        'torques': self._reward_torques(pipeline_state.data.qfrc_actuator),  # pytype: disable=attribute-error\n",
        "        'action_rate': self._reward_action_rate(action, state.info['last_act']),\n",
        "        'stand_still': self._reward_stand_still(\n",
        "            state.info['command'], joint_angles,\n",
        "        ),\n",
        "        'feet_air_time': self._reward_feet_air_time(\n",
        "            state.info['feet_air_time'],\n",
        "            first_contact,\n",
        "            state.info['command'],\n",
        "        ),\n",
        "        'foot_slip': self._reward_foot_slip(pipeline_state, contact_filt_cm),\n",
        "        'termination': self._reward_termination(done, state.info['step']),\n",
        "    }\n",
        "    rewards = {\n",
        "        k: v * self.reward_config.rewards.scales[k] for k, v in rewards.items()\n",
        "    }\n",
        "    reward = jp.clip(sum(rewards.values()) * self.dt, 0.0, 10000.0)\n",
        "\n",
        "    # state management\n",
        "    state.info['kick'] = kick\n",
        "    state.info['last_act'] = action\n",
        "    state.info['last_vel'] = joint_vel\n",
        "    state.info['feet_air_time'] *= ~contact_filt_mm\n",
        "    state.info['last_contact'] = contact\n",
        "    state.info['rewards'] = rewards\n",
        "    state.info['step'] += 1\n",
        "    state.info['rng'] = rng\n",
        "\n",
        "    # sample new command if more than 500 timesteps achieved\n",
        "    state.info['command'] = jp.where(\n",
        "        state.info['step'] > 500,\n",
        "        self.sample_command(cmd_rng),\n",
        "        state.info['command'],\n",
        "    )\n",
        "    # reset the step counter when done\n",
        "    state.info['step'] = jp.where(\n",
        "        done | (state.info['step'] > 500), 0, state.info['step']\n",
        "    )\n",
        "\n",
        "    # log total displacement as a proxy metric\n",
        "    state.metrics['total_dist'] = math.normalize(x.pos[self._torso_idx - 1])[1]\n",
        "    state.metrics.update(state.info['rewards'])\n",
        "\n",
        "    done = jp.float32(done)\n",
        "    state = state.replace(\n",
        "        pipeline_state=pipeline_state, obs=obs, reward=reward, done=done\n",
        "    )\n",
        "    return state\n",
        "\n",
        "  def _get_obs(\n",
        "      self,\n",
        "      pipeline_state: base.State,\n",
        "      state_info: dict[str, Any],\n",
        "      obs_history: jax.Array,\n",
        "  ) -> jax.Array:\n",
        "    inv_torso_rot = math.quat_inv(pipeline_state.x.rot[0])\n",
        "    local_rpyrate = math.rotate(pipeline_state.xd.ang[0], inv_torso_rot)\n",
        "\n",
        "    obs = jp.concatenate([\n",
        "        jp.array([local_rpyrate[2]]) * 0.25,                 # yaw rate\n",
        "        math.rotate(jp.array([0, 0, -1]), inv_torso_rot),    # projected gravity\n",
        "        state_info['command'] * jp.array([2.0, 2.0, 0.25]),  # command\n",
        "        pipeline_state.q[7:] - self._default_pose,           # motor angles\n",
        "        state_info['last_act'],                              # last action\n",
        "    ])\n",
        "\n",
        "    # clip, noise\n",
        "    obs = jp.clip(obs, -100.0, 100.0) + self._obs_noise * jax.random.uniform(\n",
        "        state_info['rng'], obs.shape, minval=-1, maxval=1\n",
        "    )\n",
        "    # stack observations through time\n",
        "    obs = jp.roll(obs_history, obs.size).at[:obs.size].set(obs)\n",
        "\n",
        "    return obs\n",
        "\n",
        "  # ------------ reward functions----------------\n",
        "  def _reward_lin_vel_z(self, xd: Motion) -> jax.Array:\n",
        "    # Penalize z axis base linear velocity\n",
        "    return jp.square(xd.vel[0, 2])\n",
        "\n",
        "  def _reward_ang_vel_xy(self, xd: Motion) -> jax.Array:\n",
        "    # Penalize xy axes base angular velocity\n",
        "    return jp.sum(jp.square(xd.ang[0, :2]))\n",
        "\n",
        "  def _reward_orientation(self, x: Transform) -> jax.Array:\n",
        "    # Penalize non flat base orientation\n",
        "    up = jp.array([0.0, 0.0, 1.0])\n",
        "    rot_up = math.rotate(up, x.rot[0])\n",
        "    return jp.sum(jp.square(rot_up[:2]))\n",
        "\n",
        "  def _reward_torques(self, torques: jax.Array) -> jax.Array:\n",
        "    # Penalize torques\n",
        "    return jp.sqrt(jp.sum(jp.square(torques))) + jp.sum(jp.abs(torques))\n",
        "\n",
        "  def _reward_action_rate(\n",
        "      self, act: jax.Array, last_act: jax.Array\n",
        "  ) -> jax.Array:\n",
        "    # Penalize changes in actions\n",
        "    return jp.sum(jp.square(act - last_act))\n",
        "\n",
        "  def _reward_tracking_lin_vel(\n",
        "      self, commands: jax.Array, x: Transform, xd: Motion\n",
        "  ) -> jax.Array:\n",
        "    # Tracking of linear velocity commands (xy axes)\n",
        "    local_vel = math.rotate(xd.vel[0], math.quat_inv(x.rot[0]))\n",
        "    lin_vel_error = jp.sum(jp.square(commands[:2] - local_vel[:2]))\n",
        "    lin_vel_reward = jp.exp(\n",
        "        -lin_vel_error / self.reward_config.rewards.tracking_sigma\n",
        "    )\n",
        "    return lin_vel_reward\n",
        "\n",
        "  def _reward_tracking_ang_vel(\n",
        "      self, commands: jax.Array, x: Transform, xd: Motion\n",
        "  ) -> jax.Array:\n",
        "    # Tracking of angular velocity commands (yaw)\n",
        "    base_ang_vel = math.rotate(xd.ang[0], math.quat_inv(x.rot[0]))\n",
        "    ang_vel_error = jp.square(commands[2] - base_ang_vel[2])\n",
        "    return jp.exp(-ang_vel_error / self.reward_config.rewards.tracking_sigma)\n",
        "\n",
        "  def _reward_feet_air_time(\n",
        "      self, air_time: jax.Array, first_contact: jax.Array, commands: jax.Array\n",
        "  ) -> jax.Array:\n",
        "    # Reward air time.\n",
        "    rew_air_time = jp.sum((air_time - 0.1) * first_contact)\n",
        "    rew_air_time *= (\n",
        "        math.normalize(commands[:2])[1] > 0.05\n",
        "    )  # no reward for zero command\n",
        "    return rew_air_time\n",
        "\n",
        "  def _reward_stand_still(\n",
        "      self,\n",
        "      commands: jax.Array,\n",
        "      joint_angles: jax.Array,\n",
        "  ) -> jax.Array:\n",
        "    # Penalize motion at zero commands\n",
        "    return jp.sum(jp.abs(joint_angles - self._default_pose)) * (\n",
        "        math.normalize(commands[:2])[1] < 0.1\n",
        "    )\n",
        "\n",
        "  def _reward_foot_slip(\n",
        "      self, pipeline_state: base.State, contact_filt: jax.Array\n",
        "  ) -> jax.Array:\n",
        "    # get velocities at feet which are offset from lower legs\n",
        "    # pytype: disable=attribute-error\n",
        "    pos = pipeline_state.data.site_xpos[self._feet_site_id]  # feet position\n",
        "    feet_offset = pos - pipeline_state.data.xpos[self._lower_leg_body_id]\n",
        "    # pytype: enable=attribute-error\n",
        "    offset = base.Transform.create(pos=feet_offset)\n",
        "    foot_indices = self._lower_leg_body_id - 1  # we got rid of the world body\n",
        "    foot_vel = offset.vmap().do(pipeline_state.xd.take(foot_indices)).vel\n",
        "\n",
        "    # Penalize large feet velocity for feet that are in contact with the ground.\n",
        "    return jp.sum(jp.square(foot_vel[:, :2]) * contact_filt.reshape((-1, 1)))\n",
        "\n",
        "  def _reward_termination(self, done: jax.Array, step: jax.Array) -> jax.Array:\n",
        "    return done & (step < 500)\n",
        "\n",
        "  def render(\n",
        "      self, trajectory: List[base.State], camera: str | None = None\n",
        "  ) -> Sequence[np.ndarray]:\n",
        "    camera = camera or 'track'\n",
        "    return super().render(trajectory, camera)\n",
        "\n",
        "envs.register_environment('barkour', BarkourEnv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi_yrcz-Qp3W"
      },
      "outputs": [],
      "source": [
        "env_name = 'barkour'\n",
        "env = envs.get_environment(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxaNFP9mA23H"
      },
      "source": [
        "## Train Policy\n",
        "\n",
        "To train a policy with domain randomization, we pass in the domain randomization function into the brax train function; brax will call the domain randomization function when rolling out episodes. Training the quadruped takes  8-9 minutes on a Tesla A100 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHJCbESGA7Rk"
      },
      "outputs": [],
      "source": [
        "make_networks_factory = functools.partial(\n",
        "    ppo_networks.make_ppo_networks,\n",
        "        policy_hidden_layer_sizes=(128, 128, 128, 128))\n",
        "train_fn = functools.partial(\n",
        "      ppo.train, num_timesteps=100_000_000, num_evals=10,\n",
        "      reward_scaling=1, episode_length=1000, normalize_observations=True,\n",
        "      action_repeat=1, unroll_length=20, num_minibatches=32,\n",
        "      num_updates_per_batch=4, discounting=0.97, learning_rate=3.0e-4,\n",
        "      entropy_cost=1e-2, num_envs=8192, batch_size=256,\n",
        "      network_factory=make_networks_factory,\n",
        "      randomization_fn=domain_randomize, seed=0)\n",
        "\n",
        "x_data = []\n",
        "y_data = []\n",
        "ydataerr = []\n",
        "times = [datetime.now()]\n",
        "max_y, min_y = 40, 0\n",
        "\n",
        "# Reset environments since internals may be overwritten by tracers from the\n",
        "# domain randomization function.\n",
        "env = envs.get_environment(env_name)\n",
        "eval_env = envs.get_environment(env_name)\n",
        "make_inference_fn, params, _= train_fn(environment=env,\n",
        "                                       progress_fn=progress,\n",
        "                                       eval_env=eval_env)\n",
        "\n",
        "print(f'time to jit: {times[1] - times[0]}')\n",
        "print(f'time to train: {times[-1] - times[1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yge-CGP5JoO"
      },
      "outputs": [],
      "source": [
        "# Save and reload params.\n",
        "model_path = '/tmp/mjx_brax_quadruped_policy'\n",
        "model.save_params(model_path, params)\n",
        "params = model.load_params(model_path)\n",
        "\n",
        "inference_fn = make_inference_fn(params)\n",
        "jit_inference_fn = jax.jit(inference_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L01IrN4oCIkC"
      },
      "source": [
        "## Visualize Policy\n",
        "\n",
        "For the Barkour Quadruped, the joystick commands can be set through `x_vel`, `y_vel`, and `ang_vel`. `x_vel` and `y_vel` define the linear forward and sideways velocities with respect to the quadruped torso. `ang_vel` defines the angular velocity of the torso in the z direction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTbpEtXnEecd"
      },
      "outputs": [],
      "source": [
        "eval_env = envs.get_environment(env_name)\n",
        "\n",
        "jit_reset = jax.jit(eval_env.reset)\n",
        "jit_step = jax.jit(eval_env.step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRRN-8L-BivZ"
      },
      "outputs": [],
      "source": [
        "# @markdown Commands **only used for Barkour Env**:\n",
        "x_vel = 1.0  #@param {type: \"number\"}\n",
        "y_vel = 0.0  #@param {type: \"number\"}\n",
        "ang_vel = -0.5  #@param {type: \"number\"}\n",
        "\n",
        "the_command = jp.array([x_vel, y_vel, ang_vel])\n",
        "\n",
        "# initialize the state\n",
        "rng = jax.random.PRNGKey(0)\n",
        "state = jit_reset(rng)\n",
        "state.info['command'] = the_command\n",
        "rollout = [state.pipeline_state]\n",
        "\n",
        "# grab a trajectory\n",
        "n_steps = 500\n",
        "render_every = 2\n",
        "\n",
        "for i in range(n_steps):\n",
        "  act_rng, rng = jax.random.split(rng)\n",
        "  ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
        "  state = jit_step(state, ctrl)\n",
        "  rollout.append(state.pipeline_state)\n",
        "\n",
        "media.show_video(\n",
        "    eval_env.render(rollout[::render_every], camera='track'),\n",
        "    fps=1.0 / eval_env.dt / render_every)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD6H6WD0915X"
      },
      "source": [
        "We can also render the rollout using the Brax renderer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7jqv08X95u4"
      },
      "outputs": [],
      "source": [
        "HTML(html.render(eval_env.brax_sys, rollout))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "gpuType": "A100",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "11cFRVCJ8Kn71tlQFbFcw4JzQZ00F8BRG",
          "timestamp": 1704355889284
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
