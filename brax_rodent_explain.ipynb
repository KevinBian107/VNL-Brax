{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation for Explaining Brax Rodent\n",
    "Theoretical understanding comes from mathematical maturity and implications directly comes from data science maturity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "0. You need to create an virtual environment specifically just for this first and install all the dependencies (python venv). Do everything once you are in the environmnt that you want to run your program in\n",
    "1. `git clone \"https://github.com/talmolab/Brax-Rodent-Run.git\"`\n",
    "2. `pip install -r requirements.txt`\n",
    "    - mujoco\n",
    "    - mujoco-mjx\n",
    "    - brax\n",
    "    - wandb\n",
    "    - mediapy\n",
    "3. `pip install -U \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html`\n",
    "    - specifically for jax\n",
    "4. `pip install -U numba`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import functools\n",
    "from IPython.display import HTML\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Sequence, Tuple, Union\n",
    "import wandb\n",
    "\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Base, Motion, Transform\n",
    "from brax.envs.base import Env, MjxEnv, State\n",
    "from brax.mjx.base import State as MjxState\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.io import html, mjcf, model\n",
    "\n",
    "from etils import epath\n",
    "from flax import struct\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapy as media\n",
    "from ml_collections import config_dict\n",
    "import mujoco\n",
    "from mujoco import mjx\n",
    "\n",
    "import yaml\n",
    "from typing import List, Dict, Text\n",
    "from tqdm import tqdm\n",
    "from IPython.display import HTML, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Rodent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramter loading\n",
    "def load_params(param_path: Text) -> Dict:\n",
    "    with open(param_path, \"rb\") as file:\n",
    "        params = yaml.safe_load(file)\n",
    "    return params\n",
    "\n",
    "params = load_params(\"params/params.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Rodent Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rodent(MjxEnv):\n",
    "    def __init__(\n",
    "            self,\n",
    "            forward_reward_weight=5,\n",
    "            ctrl_cost_weight=0.1,\n",
    "            healthy_reward=0.5,\n",
    "            terminate_when_unhealthy=False,\n",
    "            healthy_z_range=(0.2, 1.0), # from ant documentation\n",
    "            reset_noise_scale=1e-2,\n",
    "            exclude_current_positions_from_observation=False,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        params = load_params(\"params/params.yaml\")\n",
    "        mj_model = mujoco.MjModel.from_xml_path(params[\"XML_PATH\"])\n",
    "        mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
    "        mj_model.opt.iterations = 6\n",
    "        mj_model.opt.ls_iterations = 6\n",
    "\n",
    "        physics_steps_per_control_step = 5\n",
    "        kwargs['n_frames'] = kwargs.get(\n",
    "            'n_frames', physics_steps_per_control_step)\n",
    "\n",
    "        super().__init__(model=mj_model, **kwargs)\n",
    "\n",
    "        self._forward_reward_weight = forward_reward_weight\n",
    "        self._ctrl_cost_weight = ctrl_cost_weight\n",
    "        self._healthy_reward = healthy_reward\n",
    "        self._terminate_when_unhealthy = terminate_when_unhealthy\n",
    "        self._healthy_z_range = healthy_z_range\n",
    "        self._reset_noise_scale = reset_noise_scale\n",
    "        self._exclude_current_positions_from_observation = (\n",
    "            exclude_current_positions_from_observation\n",
    "        )\n",
    "\n",
    "    def reset(self, rng: jp.ndarray) -> State:\n",
    "        \"\"\"Resets the environment to an initial state.\"\"\"\n",
    "        rng, rng1, rng2 = jax.random.split(rng, 3)\n",
    "\n",
    "        low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
    "        qpos = self.sys.qpos0 + jax.random.uniform(\n",
    "            rng1, (self.sys.nq,), minval=low, maxval=hi\n",
    "        )\n",
    "        qvel = jax.random.uniform(\n",
    "            rng2, (self.sys.nv,), minval=low, maxval=hi\n",
    "        )\n",
    "\n",
    "        data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "        obs = self._get_obs(data.data, jp.zeros(self.sys.nu))\n",
    "        reward, done, zero = jp.zeros(3)\n",
    "        metrics = {\n",
    "            'forward_reward': zero,\n",
    "            'reward_linvel': zero,\n",
    "            'reward_quadctrl': zero,\n",
    "            'reward_alive': zero,\n",
    "            'x_position': zero,\n",
    "            'y_position': zero,\n",
    "            'distance_from_origin': zero,\n",
    "            'x_velocity': zero,\n",
    "            'y_velocity': zero,\n",
    "        }\n",
    "        return State(data, obs, reward, done, metrics)\n",
    "\n",
    "    def step(self, state: State, action: jp.ndarray) -> State:\n",
    "        \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
    "        \n",
    "        # initial data\n",
    "        data0 = state.pipeline_state\n",
    "\n",
    "        # This is the step, which this environment is wrapped and directly for ppo to use\n",
    "        data = self.pipeline_step(data0, action)\n",
    "        \n",
    "        # based on the timestep simulation, calculate the rewards\n",
    "        com_before = data0.data.subtree_com[1]\n",
    "        com_after = data.data.subtree_com[1]\n",
    "\n",
    "        velocity = (com_after - com_before) / self.dt\n",
    "        forward_reward = self._forward_reward_weight * velocity[0]\n",
    "\n",
    "        min_z, max_z = self._healthy_z_range\n",
    "        is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
    "        is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
    "        \n",
    "        if self._terminate_when_unhealthy:\n",
    "            healthy_reward = self._healthy_reward\n",
    "        else:\n",
    "            healthy_reward = self._healthy_reward * is_healthy\n",
    "\n",
    "        ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
    "\n",
    "        obs = self._get_obs(data.data, action)\n",
    "        reward = forward_reward + healthy_reward - ctrl_cost\n",
    "        \n",
    "        # terminates when unhealthy\n",
    "        done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
    "        state.metrics.update(\n",
    "            forward_reward=forward_reward,\n",
    "            reward_linvel=forward_reward,\n",
    "            reward_quadctrl=-ctrl_cost,\n",
    "            reward_alive=healthy_reward,\n",
    "            x_position=com_after[0],\n",
    "            y_position=com_after[1],\n",
    "            distance_from_origin=jp.linalg.norm(com_after),\n",
    "            x_velocity=velocity[0],\n",
    "            y_velocity=velocity[1],\n",
    "        )\n",
    "\n",
    "        return state.replace(\n",
    "            pipeline_state=data, obs=obs, reward=reward, done=done\n",
    "        )\n",
    "\n",
    "    def _get_obs(\n",
    "            self, data: mjx.Data, action: jp.ndarray\n",
    "    ) -> jp.ndarray:\n",
    "        \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
    "        position = data.qpos\n",
    "        if self._exclude_current_positions_from_observation:\n",
    "            position = position[2:]\n",
    "            \n",
    "        # external_contact_forces are excluded\n",
    "        return jp.concatenate([\n",
    "            position,\n",
    "            data.qvel,\n",
    "            data.cinert[1:].ravel(),\n",
    "            data.cvel[1:].ravel(),\n",
    "            data.qfrc_actuator,\n",
    "        ])\n",
    "\n",
    "envs.register_environment('rodent', Rodent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the environment\n",
    "env_name = 'rodent'\n",
    "env = envs.get_environment(env_name)\n",
    "\n",
    "# define the jit reset/step functions\n",
    "# jit stands for \"just in time\", which is what jax uses to accelerate computation by converting it to a different format\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the state\n",
    "state = jit_reset(jax.random.PRNGKey(0))\n",
    "rollout = [state.pipeline_state]\n",
    "\n",
    "# grab a trajectory\n",
    "# tqdm time bar\n",
    "for i in tqdm(range(10)):\n",
    "  ctrl = -0.1 * jp.ones(env.sys.nu)\n",
    "  state = jit_step(state, ctrl)\n",
    "  rollout.append(state.pipeline_state)\n",
    "\n",
    "media.show_video(env.render(rollout, camera='side'), fps=1.0 / env.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = jit_reset(jax.random.PRNGKey(0))\n",
    "HTML(html.render(env.sys, [state.pipeline_state]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "This is a direct easy view of some of the most important hyperparametyer tuning that we need todo. Also visit PPO Documentation for more details (https://github.com/google/brax/blob/main/brax/training/agents/ppo/train.py).\n",
    "1. `num_env`:\n",
    "    - Number of environment is refering to the number of parrallel environment that the agent is traine on. In another word, it is how many instances of the registered environment that have being activated and trained in the same time.\n",
    "    - It creates more robust and diverse policy\n",
    "    - Agent learns quicker because it gathers experiences from all of them and just choose the best policy\n",
    "2. `num_timesteps`:\n",
    "    - Total number of interactions that will happen between the agent and environment\n",
    "3. `eval_every`:\n",
    "    - eval_every is the learning time, it is how often do we update the policy parameter. This is the same with the horizon idea N in the pytorch simple version PPO we implemented\n",
    "4. `episode_length`:\n",
    "    - episode length is the number of timesteps that constitute one episode. In here it would be 1000 * 10_000 episodes.\n",
    "5. `num_evals`:\n",
    "    - In total how many evals there are, should be num_timesteps/eval_every\n",
    "6. `batch_size`:\n",
    "    - Batch size is the number of samples extracted from the \"replay buffer\" for calling as previous experiences everytime during bellman equation optimization for 1 batch.\n",
    "    - This 1 batch is used for 1 SGD only, we SGD multiple times with multiple subsamples or \"mini batches\"\n",
    "    - In brax implementation of ppo, it doesn't have an replay buffer, but the idea is the same.\n",
    "7. `num_minibatches`:\n",
    "    - This is how many batches there are by splitting all data in the replay buffer to little chunks\n",
    "    - This is also how many times the stochastic gradient descent is run\n",
    "    - num_batch * batch_size = all_data\n",
    "    - as replay buffer increase, num_minibatches is the same, each batch_size increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"env_name\": env_name,\n",
    "    \"algo_name\": \"ppo\",\n",
    "    \"task_name\": \"run\",\n",
    "    \"num_envs\": 2048,\n",
    "    \"num_timesteps\": 10_000_000,\n",
    "    \"eval_every\": 10_000,\n",
    "    \"episode_length\": 1000,\n",
    "    \"num_evals\": 1000,\n",
    "    \"batch_size\": 512,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"terminate_when_unhealthy\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Record on Wandb, and Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = functools.partial(\n",
    "    ppo.train, num_timesteps=config[\"num_timesteps\"], num_evals=int(config[\"num_timesteps\"]/config[\"eval_every\"]),\n",
    "    reward_scaling=0.1, episode_length=config[\"episode_length\"], normalize_observations=True, action_repeat=1,\n",
    "    unroll_length=10, num_minibatches=8, num_updates_per_batch=4,\n",
    "    discounting=0.98, learning_rate=config[\"learning_rate\"], entropy_cost=1e-3, num_envs=config[\"num_envs\"],\n",
    "    batch_size=config[\"batch_size\"], seed=0)\n",
    "\n",
    "# Saving everything to Wandb\n",
    "run = wandb.init(project=\"vnl\", config=config)\n",
    "\n",
    "wandb.run.name = f\"{config['env_name']}_{config['task_name']}_{config['algo_name']}_brax\"\n",
    "\n",
    "def wandb_progress(num_steps, metrics):\n",
    "    metrics[\"num_steps\"] = num_steps\n",
    "    wandb.log(metrics)\n",
    "    \n",
    "# Making inference\n",
    "make_inference_fn, params, _= train_fn(environment=env, progress_fn=wandb_progress) # diectly use wandb as progress function\n",
    "\n",
    "#@title Save Model\n",
    "model_path = '/cps/brax_ppo_rodent_run'\n",
    "model.save_params(model_path, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
