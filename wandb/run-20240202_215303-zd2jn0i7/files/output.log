
{'eval/walltime': 1013.777172088623, 'eval/episode_distance_from_origin': Array(0., dtype=float32), 'eval/episode_forward_reward': Array(0., dtype=float32), 'eval/episode_reward': Array(4.2, dtype=float32), 'eval/episode_reward_alive': Array(5., dtype=float32), 'eval/episode_reward_linvel': Array(0., dtype=float32), 'eval/episode_reward_quadctrl': Array(-0.8, dtype=float32), 'eval/episode_x_position': Array(0., dtype=float32), 'eval/episode_x_velocity': Array(0., dtype=float32), 'eval/episode_y_position': Array(0., dtype=float32), 'eval/episode_y_velocity': Array(0., dtype=float32), 'eval/episode_distance_from_origin_std': Array(0., dtype=float32), 'eval/episode_forward_reward_std': Array(0., dtype=float32), 'eval/episode_reward_std': Array(0., dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(0., dtype=float32), 'eval/episode_reward_quadctrl_std': Array(0., dtype=float32), 'eval/episode_x_position_std': Array(0., dtype=float32), 'eval/episode_x_velocity_std': Array(0., dtype=float32), 'eval/episode_y_position_std': Array(0., dtype=float32), 'eval/episode_y_velocity_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/epoch_eval_time': 1013.777172088623, 'eval/sps': 126.26048753523364, 'num_steps': 0}
{'eval/walltime': 1971.919399023056, 'training/sps': 836.3498907613324, 'training/walltime': 18022.696202278137, 'training/entropy_loss': Array(0.80684847, dtype=float32), 'training/policy_loss': Array(0.2807547, dtype=float32), 'training/total_loss': Array(63600.5, dtype=float32), 'training/v_loss': Array(63599.418, dtype=float32), 'eval/episode_distance_from_origin': Array(0., dtype=float32), 'eval/episode_forward_reward': Array(0., dtype=float32), 'eval/episode_reward': Array(4.2, dtype=float32), 'eval/episode_reward_alive': Array(5., dtype=float32), 'eval/episode_reward_linvel': Array(0., dtype=float32), 'eval/episode_reward_quadctrl': Array(-0.8, dtype=float32), 'eval/episode_x_position': Array(0., dtype=float32), 'eval/episode_x_velocity': Array(0., dtype=float32), 'eval/episode_y_position': Array(0., dtype=float32), 'eval/episode_y_velocity': Array(0., dtype=float32), 'eval/episode_distance_from_origin_std': Array(0., dtype=float32), 'eval/episode_forward_reward_std': Array(0., dtype=float32), 'eval/episode_reward_std': Array(0., dtype=float32), 'eval/episode_reward_alive_std': Array(0., dtype=float32), 'eval/episode_reward_linvel_std': Array(0., dtype=float32), 'eval/episode_reward_quadctrl_std': Array(0., dtype=float32), 'eval/episode_x_position_std': Array(0., dtype=float32), 'eval/episode_x_velocity_std': Array(0., dtype=float32), 'eval/episode_y_position_std': Array(0., dtype=float32), 'eval/episode_y_velocity_std': Array(0., dtype=float32), 'eval/avg_episode_length': Array(1., dtype=float32), 'eval/epoch_eval_time': 958.142226934433, 'eval/sps': 133.59185766139834, 'num_steps': 15073280}
Traceback (most recent call last):
  File "/home/jovyan/Brax-Rodent-Run/serevr_run.py", line 290, in <module>
    make_inference_fn, params, _ = train_fn(environment=env, progress_fn=wandb_progress, policy_params_fn=policy_params_fn)
  File "/opt/conda/lib/python3.10/site-packages/brax/training/agents/ppo/train.py", line 443, in train
    policy_params_fn(current_step, make_policy, params)
  File "/home/jovyan/Brax-Rodent-Run/serevr_run.py", line 287, in policy_params_fn
    os.makedirs(model_path, exist_ok=True)
NameError: name 'os' is not defined