{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19371b8-9c17-4b5a-98ec-1d756117011f",
   "metadata": {},
   "source": [
    "## Initialize the Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f8270c-848f-4b7c-9a2c-8ce6f5758edc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import functools\n",
    "from IPython.display import HTML\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Sequence, Tuple, Union, Optional\n",
    "import wandb\n",
    "\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Base, Motion, Transform\n",
    "from brax.envs.base import Env, MjxEnv, State\n",
    "from brax.mjx.base import State as MjxState\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.io import html, mjcf, model\n",
    "\n",
    "from etils import epath\n",
    "from flax import struct\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapy as media\n",
    "from ml_collections import config_dict\n",
    "import mujoco\n",
    "from mujoco import mjx\n",
    "import os\n",
    "\n",
    "import yaml\n",
    "from typing import List, Dict, Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2d15035-d974-402d-9b6a-f86e2a29e37c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_params(param_path: Text) -> Dict:\n",
    "    with open(param_path, \"rb\") as file:\n",
    "        params = yaml.safe_load(file)\n",
    "    return params\n",
    "\n",
    "\n",
    "params = load_params(\"params/params.yaml\")\n",
    "\n",
    "class Humanoid(MjxEnv):\n",
    "  '''\n",
    "  This is greatly coustomizable of what reward you want to give: reward engineering\n",
    "  '''\n",
    "  def __init__(\n",
    "      self,\n",
    "      forward_reward_weight=1.25,\n",
    "      ctrl_cost_weight=0.1,\n",
    "      healthy_reward=5.0,\n",
    "      terminate_when_unhealthy=True,\n",
    "      healthy_z_range=(1.0, 1.5),\n",
    "      reset_noise_scale=1e-2,\n",
    "      exclude_current_positions_from_observation=True,\n",
    "      **kwargs,):\n",
    "    '''\n",
    "    Defining initilization of the agent\n",
    "    '''\n",
    "\n",
    "    path = epath.Path(epath.resource_path('mujoco')) / ('mjx/benchmark/model/humanoid')\n",
    "    mj_model = mujoco.MjModel.from_xml_path((path / 'humanoid.xml').as_posix())\n",
    "\n",
    "    # solver is an optimization system\n",
    "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
    "\n",
    "    #Iterations for solver\n",
    "    mj_model.opt.iterations = 6\n",
    "    mj_model.opt.ls_iterations = 6\n",
    "\n",
    "    # Defult framne to be 5, but can self define in kwargs\n",
    "    physics_steps_per_control_step = 5\n",
    "    kwargs['n_frames'] = kwargs.get(\n",
    "        'n_frames', physics_steps_per_control_step)\n",
    "\n",
    "    # Parents inheritence from MjxEnv class\n",
    "    super().__init__(model=mj_model, **kwargs)\n",
    "\n",
    "    # Global vraiable for later calling them\n",
    "    self._forward_reward_weight = forward_reward_weight\n",
    "    self._ctrl_cost_weight = ctrl_cost_weight\n",
    "    self._healthy_reward = healthy_reward\n",
    "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
    "    self._healthy_z_range = healthy_z_range\n",
    "    self._reset_noise_scale = reset_noise_scale\n",
    "    self._exclude_current_positions_from_observation = (exclude_current_positions_from_observation)\n",
    "\n",
    "  def reset(self, rng: jp.ndarray) -> State:\n",
    "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
    "\n",
    "    #Creating randome keys\n",
    "    #rng = random number generator key for starting random initiation\n",
    "    rng, rng1, rng2 = jax.random.split(rng, 3)\n",
    "\n",
    "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
    "\n",
    "    #Vectors of generalized joint position in the configuration space\n",
    "    qpos = self.sys.qpos0 + jax.random.uniform(\n",
    "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
    "    )\n",
    "\n",
    "    #Vectors of generalized joint velocities in the configuration space\n",
    "    qvel = jax.random.uniform(\n",
    "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
    "    )\n",
    "\n",
    "    data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "    #Reset everything\n",
    "    obs = self._get_obs(data.data, jp.zeros(self.sys.nu))\n",
    "    reward, done, zero = jp.zeros(3)\n",
    "    metrics = {\n",
    "        'forward_reward': zero,\n",
    "        'reward_linvel': zero,\n",
    "        'reward_quadctrl': zero,\n",
    "        'reward_alive': zero,\n",
    "        'x_position': zero,\n",
    "        'y_position': zero,\n",
    "        'distance_from_origin': zero,\n",
    "        'x_velocity': zero,\n",
    "        'y_velocity': zero,\n",
    "    }\n",
    "    return State(data, obs, reward, done, metrics)\n",
    "\n",
    "  def step(self, state: State, action: jp.ndarray) -> State:\n",
    "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
    "    #Previous Pipeline\n",
    "    data0 = state.pipeline_state\n",
    "\n",
    "    #Current pipeline state, step 1\n",
    "    data = self.pipeline_step(data0, action)\n",
    "\n",
    "    #Running forward (Velocity)\n",
    "    com_before = data0.data.subtree_com[1]\n",
    "    com_after = data.data.subtree_com[1]\n",
    "    velocity = (com_after - com_before) / self.dt\n",
    "    forward_reward = self._forward_reward_weight * velocity[0] * 2\n",
    "\n",
    "    #Height being healthy\n",
    "    min_z, max_z = self._healthy_z_range\n",
    "    is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
    "    is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
    "\n",
    "    #Termination condition\n",
    "    if self._terminate_when_unhealthy:\n",
    "      healthy_reward = self._healthy_reward\n",
    "    else:\n",
    "      healthy_reward = self._healthy_reward * is_healthy\n",
    "\n",
    "    #Control quad cost\n",
    "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
    "\n",
    "    #Feedback from env\n",
    "    obs = self._get_obs(data.data, action)\n",
    "    reward = forward_reward + healthy_reward - ctrl_cost\n",
    "\n",
    "    #Termination State\n",
    "    done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
    "\n",
    "    state.metrics.update(\n",
    "        forward_reward=forward_reward,\n",
    "        reward_linvel=forward_reward,\n",
    "        reward_quadctrl=-ctrl_cost,\n",
    "        reward_alive=healthy_reward,\n",
    "        x_position=com_after[0],\n",
    "        y_position=com_after[1],\n",
    "        distance_from_origin=jp.linalg.norm(com_after),\n",
    "        x_velocity=velocity[0],\n",
    "        y_velocity=velocity[1],\n",
    "    )\n",
    "\n",
    "    return state.replace(\n",
    "        pipeline_state=data, obs=obs, reward=reward, done=done\n",
    "    )\n",
    "\n",
    "  def _get_obs(self, data: mjx.Data, action: jp.ndarray) -> jp.ndarray:\n",
    "    \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
    "    position = data.qpos\n",
    "    if self._exclude_current_positions_from_observation:\n",
    "      position = position[2:]\n",
    "\n",
    "    # external_contact_forces are excluded\n",
    "    # environment observation described later\n",
    "    return jp.concatenate([\n",
    "        position,\n",
    "        data.qvel,\n",
    "        data.cinert[1:].ravel(),\n",
    "        data.cvel[1:].ravel(),\n",
    "        data.qfrc_actuator,\n",
    "    ])\n",
    "\n",
    "# Registering the environment setup in env as humanoid_mjx\n",
    "envs.register_environment('humanoid_mjx', Humanoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a329a2b-7830-4977-a063-359c93bf662f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "envs.register_environment('humanoid_mjx', Humanoid)\n",
    "env_name = 'humanoid_mjx'\n",
    "env = envs.get_environment(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d52112-95fc-49b1-97ee-60829cf172c0",
   "metadata": {},
   "source": [
    "## Load the Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5750041-5301-4e6a-9325-55fa0af5210c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from brax.training.agents.ppo import networks as brax_networks\n",
    "\n",
    "# Copy and pasted from https://github.com/google/brax/discussions/403#discussioncomment-7287194\n",
    "def make_inference_fn(\n",
    "    observation_size: int,\n",
    "    action_size: int,\n",
    "    normalize_observations: bool = False,\n",
    "    network_factory_kwargs: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    normalize = lambda x, y: x\n",
    "    ppo_network = brax_networks.make_ppo_networks(\n",
    "      observation_size,\n",
    "      action_size,\n",
    "      preprocess_observations_fn=normalize,\n",
    "      **(network_factory_kwargs or {}),\n",
    "    )\n",
    "    make_policy = brax_networks.make_inference_fn(ppo_network)\n",
    "    return make_policy\n",
    "\n",
    "make_policy = make_inference_fn(\n",
    "    observation_size=env.observation_size,\n",
    "    action_size=env.action_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d790f09-52a8-4cd0-8afc-3cc57d3918da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"./model_checkpoints/10240\"\n",
    "params = model.load_params(model_path)\n",
    "# jit_inference_fn = jax.jit(make_policy(params, deterministic=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b353a-7b58-416f-903e-bf9bbc8c12ec",
   "metadata": {},
   "source": [
    "## Render the Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39db86-dc57-41ce-89ce-3b92774d304e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# define the jit reset/step functions\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "jit_inference_fn = jax.jit(make_policy(params, deterministic=True))\n",
    "\n",
    "# initialize the state\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = jit_reset(rng)\n",
    "rollout = [state.pipeline_state]\n",
    "\n",
    "# grab a trajectory\n",
    "n_steps = 250\n",
    "render_every = 1\n",
    "\n",
    "# might becasue brax does not clip the action to the xml limit in the model\n",
    "for i in tqdm(range(n_steps)):\n",
    "    act_rng, rng = jax.random.split(rng)\n",
    "    ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "\n",
    "    state = jit_step(state, ctrl)\n",
    "    rollout.append(state.pipeline_state)\n",
    "\n",
    "    if state.done:\n",
    "        break\n",
    "\n",
    "media.show_video(env.render(rollout[::render_every], camera='side'), fps=1.0 / env.dt / render_every)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361c42c-d071-4a45-b92b-541d3db028df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
