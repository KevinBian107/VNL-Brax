{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Brax` pipeline adapted from `dm_control`\n",
    "<img src=\"https://github.com/google/brax/raw/main/docs/img/a1.gif\" width=\"200\" height=\"200\"> \n",
    "<img src=\"https://github.com/google/brax/raw/main/docs/img/humanoid_v2.gif\" width=\"200\" height=\"200\"> \n",
    "<img src=\"https://github.com/google/brax/raw/main/docs/img/ant_v2.gif\" width=\"200\" height=\"200\"> \n",
    "<img src=\"https://github.com/google/brax/raw/main/docs/img/ur5e.gif\" width=\"200\" height=\"200\"> \n",
    "\n",
    "dm_control provides a very high level abstraction of Mujoco, which are both implemented by Google Deepmind. **This repository takes a lot of time to parse through it, understand it, and see how each things fit together, but this is very needed as this is the fundamental skills that is needed in research and developing new things:**\n",
    "1. It is the first step towards understanding what you need to do to create.\n",
    "2. It is also generally a good skill to have when learning a very powerful highly abstarcted new tool, parsing through many repository classes, seeing how classes' hierchy is connected and how to find the `right level of abstraction`, seeing inherietence relationships between classes and chasing back all the way to the begining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linux GPU Rendoring\n",
    "##### These are imports, installation, and switch backends needed for `Linux GPU Rendoring`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@title Colab Only\n",
    "\n",
    "# print('brax, wandb, mujoco, and dm_control:')\n",
    "# !pip install brax\n",
    "# !pip install wandb\n",
    "# !pip install mujoco\n",
    "# !pip install dm_control\n",
    "\n",
    "# import time\n",
    "# import itertools\n",
    "# import numpy as np\n",
    "# from typing import Callable, NamedTuple, Optional, Union, List\n",
    "\n",
    "# # Graphics and plotting.\n",
    "# print('Installing mediapy:')\n",
    "# !command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
    "# !pip install -q mediapy\n",
    "# !pip install tqdm\n",
    "# import mediapy as media\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # More legible printing from numpy.\n",
    "# np.set_printoptions(precision=3, suppress=True, linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@title Check if MuJoCo installation was successful & rendoring on GPU\n",
    "# from google.colab import files\n",
    "\n",
    "# import distutils.util\n",
    "# import os\n",
    "# import subprocess\n",
    "# if subprocess.run('nvidia-smi').returncode:\n",
    "#   raise RuntimeError(\n",
    "#       'Cannot communicate with GPU. '\n",
    "#       'Make sure you are using a GPU Colab runtime. '\n",
    "#       'Go to the Runtime menu and select Choose runtime type.')\n",
    "\n",
    "# # Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
    "# # This is usually installed as part of an Nvidia driver package, but the Colab\n",
    "# # kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
    "# # (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
    "# NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "# if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "#   with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "#     f.write(\"\"\"{\n",
    "#     \"file_format_version\" : \"1.0.0\",\n",
    "#     \"ICD\" : {\n",
    "#         \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "#     }\n",
    "# }\n",
    "# \"\"\")\n",
    "\n",
    "# # Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
    "# print('Setting environment variable to use GPU rendering:')\n",
    "# %env MUJOCO_GL=egl\n",
    "\n",
    "# try:\n",
    "#   print('Checking that the installation succeeded:')\n",
    "#   import mujoco\n",
    "#   mujoco.MjModel.from_xml_string('<mujoco/>')\n",
    "# except Exception as e:\n",
    "#   raise e from RuntimeError(\n",
    "#       'Something went wrong during installation. Check the shell output above '\n",
    "#       'for more information.\\n'\n",
    "#       'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
    "#       'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
    "\n",
    "# print('Installation successful.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Building\n",
    "References to dm_control implementation of customized environment. These are the super high level libs, locomotion lib should have everything to get a rl agent running in dm control and the mjcf lib should have everything about building a connection class.\n",
    "- [dm_control locomotion lib](https://github.com/google-deepmind/dm_control/tree/main/dm_control/locomotion)\n",
    "- [dm_control mjcf lib](https://github.com/google-deepmind/dm_control/tree/main/dm_control/mjcf)\n",
    "- [dm_control lib](https://github.com/google-deepmind/dm_control/tree/main)\n",
    "- [mujoco](https://mujoco.readthedocs.io/en/3.1.1/overview.html)\n",
    "- [brax](https://github.com/google/brax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organization\n",
    "- `locomotion class`\n",
    "  - `arena subclass`\n",
    "    - i.e. corridors -> gap corridors\n",
    "  - `walkers subclass`\n",
    "    - assets (xml)\n",
    "    - i.e. rodent -> rat\n",
    "  - `tasks subclass`\n",
    "    - i.e. jump gaps\n",
    "- `mjcf class`\n",
    "  - class we want to build using idea from corridors\n",
    "- `composer class`\n",
    "  - arena.py/arena.xml provides foundation for `locomotion class`\n",
    "\n",
    "  This really took a quite long time to figure out:\n",
    "  1. We can use `tasks_subclass` by converting it to mjcf model, then MjModel\n",
    "  2. Inherentence can be used to build pur own functions needed in `task`, `arena`, and `walker` (work smart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collective Efforts\n",
    "This is a lower level abstraction compare to the previous one, but it is still really high level, calling functions and environental setup that dm_control have already implemented.\n",
    "\n",
    "The skill now is to parse through these very convoluted layers to see the `real idea` behind all the implementation and the `pipeline` of how each things are called. In this way, you can know all the way to the implementation level and then decide on which `level of abstraction` you want to stay on to achieve the purpose you need while fixing minimum things and utilize functions that others have already written to complete your goals so\n",
    ">### You can spend the time on building on top of it and actually doing more useful things to create new innovations instea of doing redendent work. Then later your work may become a function, a abstraction level, that other people call to build more works in the field. This is a collaboration project that takes a collective effort towards a common goal, this is the powerfullness of python libs, to not reinvent the wheels but use the wheels to build the car, to not recreate data structure for storing data but use the data for more things.\n",
    "\n",
    "Therefore, staying on the right level of abstraction and using implemented functions is very important (just like we used brax implementation of ppo, we can use dm_control's implementation of customized env)\n",
    "\n",
    "- Brax mjx -> Mujoco env -> modify using dm_control implemented packages -> dm_control mjcf\n",
    " - On mjx level? on dm_control implemented env sample level? on mjcf level? on xml level?\n",
    "\n",
    " ***Do notice that these things are already at the task level, which is already an execution of (model + environment), the correct level of `abstraction` should be just the background environment level, not the task level, this is the same with brax***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import functools\n",
    "from IPython.display import HTML\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Sequence, Tuple, Union\n",
    "import wandb\n",
    "\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Base, Motion, Transform\n",
    "from brax.envs.base import Env, MjxEnv, State\n",
    "from brax.mjx.base import State as MjxState\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.io import html, mjcf, model\n",
    "\n",
    "from etils import epath\n",
    "from flax import struct\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapy as media\n",
    "from ml_collections import config_dict\n",
    "import mujoco\n",
    "from mujoco import mjx\n",
    "import os\n",
    "\n",
    "import yaml\n",
    "from typing import List, Dict, Text\n",
    "from dm_control import mjcf\n",
    "\n",
    "import numpy as np\n",
    "from dm_control import composer\n",
    "from dm_control.locomotion.examples import basic_rodent_2020\n",
    "from dm_control.composer.variation import distributions\n",
    "from dm_control.locomotion.arenas import corridors as corr_arenas\n",
    "from dm_control.locomotion.tasks import corridors as corr_tasks\n",
    "from dm_control.locomotion.walkers import rodent, ant\n",
    "from dm_control import viewer\n",
    "from dm_control import mujoco as mujoco_dm\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `dm_conrol` - Task Level Running\n",
    "This code is directly implementation in the Mujoco framework:\n",
    "\n",
    "There are existing examples of the framework that calls from more basic components of the dm control lib. These examples are already super high level abstractions already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CONTROL_TIMESTEP = .02\n",
    "_PHYSICS_TIMESTEP = 0.001\n",
    "random_state=None\n",
    "\n",
    "walker = rodent.Rat(observable_options={'egocentric_camera': dict(enabled=True)})\n",
    "\n",
    "  # Build a corridor-shaped arena with gaps, where the sizes of the gaps and\n",
    "  # platforms are uniformly randomized.\n",
    "arena = corr_arenas.GapsCorridor(\n",
    "      platform_length=distributions.Uniform(.4, .8),\n",
    "      gap_length=distributions.Uniform(.05, .2),\n",
    "      corridor_width=2,\n",
    "      corridor_length=40,\n",
    "      aesthetic='outdoor_natural')\n",
    "\n",
    "  # Build a task that rewards the agent for running down the corridor at a\n",
    "  # specific velocity.\n",
    "task = corr_tasks.RunThroughCorridor(\n",
    "      walker=walker,\n",
    "      arena=arena,\n",
    "      walker_spawn_position=(5, 0, 0),\n",
    "      walker_spawn_rotation=0,\n",
    "      target_velocity=1.0,\n",
    "      contact_termination=False,\n",
    "      terminate_at_height=-0.3,\n",
    "      physics_timestep=_PHYSICS_TIMESTEP,\n",
    "      control_timestep=_CONTROL_TIMESTEP)\n",
    "\n",
    "env_composed = composer.Environment(time_limit=30,\n",
    "                              task=task,\n",
    "                              random_state=random_state,\n",
    "                              strip_singleton_obs_buffer_dim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewer.launch(environment_loader=env_composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `dm_control` Arena Backbone Level\n",
    "These are just the the backbone skeleton in the [Arena_folder](https://github.com/google-deepmind/dm_control/tree/main/dm_control/locomotion/arenas)\n",
    "- Gap Corridor -> Empty Corridor -> Corridor -> composer.arena -> xml\n",
    "- Rodent -> xml\n",
    "## First Idea:\n",
    "Making the intermediate class that connects the corridor defined environment (corridor environment is in the arena folder, not the task folder)\n",
    "* [Tasks_folder](https://github.com/google-deepmind/dm_control/tree/main/dm_control/locomotion/tasks) is directly running the algorithm already\n",
    "* [Arena_folder](https://github.com/google-deepmind/dm_control/tree/main/dm_control/locomotion/arenas) provides the actual skeleton, this is the correct level of `abstraction` that we are looking for\n",
    "* > ##### Tasks = Walker + Arena\n",
    "\n",
    "> ##### problem: we can not directly use the arena implemented by dm_control nor can we use the walker implemented by dm_control, they are too specifically designed by dm_control\n",
    "\n",
    "## Second Idea:\n",
    "1. Directly build an arena through intuitions from the corriodr class\n",
    "2. Link the rodent or humanoid xml to such environment\n",
    "3. because it is created by the mjcf class in dm_control, we can directly export it as a fix file and it cna be utilized in brax\n",
    "    - essentially we skip some of the really convoluted implementation of dm_control from `tasks -> arena + walker -> composer class -> xml`\n",
    "    - we are directly building it from scratch using `mjcf.RootElement()`\n",
    "4. mjcf model here direcly uses Mujoco, there is no need to initiate another mujoco in brax training loop, we directly pass an model into brax\n",
    "\n",
    "> ##### problem: this is way too inefficient and there should be a much smarter way working with it\n",
    "\n",
    "## Third Idea:\n",
    "1. Seems like that the Corridor class can be directly transfered into a mjcf model by `.to_mjcf_model()`\n",
    "2. Also self implemented a Corridor class that is directly based on `mjcf.RootElement()` and return the model using `.out()` functions\n",
    "3. Try to inherent directly from the Corridor class and assing some more things\n",
    "\n",
    "> ##### problem: so far Inherent class idea is implemented quite well, but binding not assigned\n",
    "\n",
    "## Forth Idea:\n",
    "1. Directly wrap evrything once `binding` is down with the agent and also the environment at the `tasks` level\n",
    "2. task level can also be exported as an mjcf_model file and the same storing as in MjModel once ptr, same with the brax humanoid that was previously implemented in the training loop.\n",
    "3. Created inherietence from task module and the directly bind it to an existing waler construct file (fix ours later)\n",
    "\n",
    "> ##### problem: this `task wrapper binded model` has friction setup that Mujoco does not support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directly Inherent Class for Brax\n",
    "### Primary Goal:\n",
    "1. Create direct inheritence that from dm_control (`walker, arena, tasks -> wrapper to physics`)\n",
    "2. Build a pipeline for using customization functions from dm_control for brax training by implementing particular classes that we can change to fit our our needs for the `arena`, `walker`, and `tasks binding` (binds walker and arena)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gap_Vnl(corr_arenas.GapsCorridor):\n",
    "    def _build(self, corridor_width, corridor_length, visible_side_planes, aesthetic, platform_length, gap_length):\n",
    "        super()._build(corridor_width=corridor_width,\n",
    "                       corridor_length=corridor_length,\n",
    "                       visible_side_planes=visible_side_planes,\n",
    "                       aesthetic = aesthetic,\n",
    "                       platform_length = platform_length,\n",
    "                       gap_length = gap_length)\n",
    "\n",
    "        # self.walker = mjcf.from_path(\"./models/rodent_dm_control.xml\")\n",
    "        # # Initiate walker\n",
    "        # self.spawn_pos = (0, 0, 0)\n",
    "        # self.spawn_site =  self._mjcf_root.worldbody.add('site', pos=self.spawn_pos)\n",
    "        # self.spawn_site.attach(self.walker).add('freejoint')\n",
    "    \n",
    "    def regenerate(self, random_state):\n",
    "        super().regenerate(random_state)\n",
    "\n",
    "\n",
    "class Task_Vnl(corr_tasks.RunThroughCorridor):\n",
    "    def __init__(self,\n",
    "               walker,\n",
    "               arena,\n",
    "               walker_spawn_position=(0, 0, 0),\n",
    "               walker_spawn_rotation=None,\n",
    "               target_velocity=3.0,\n",
    "               contact_termination=True,\n",
    "               terminate_at_height=-0.5,\n",
    "               physics_timestep=0.005,\n",
    "               control_timestep=0.025):\n",
    "        super().__init__(walker=walker,\n",
    "                         arena=arena,\n",
    "                         walker_spawn_position=walker_spawn_position,\n",
    "                         walker_spawn_rotation=walker_spawn_rotation,\n",
    "                         target_velocity=target_velocity,\n",
    "                         contact_termination=contact_termination,\n",
    "                         terminate_at_height=terminate_at_height,\n",
    "                         physics_timestep=physics_timestep,\n",
    "                         control_timestep=control_timestep)\n",
    "        \n",
    "# Implement Walker Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Gap_Vnl(platform_length=distributions.Uniform(.4, .8),\n",
    "      gap_length=distributions.Uniform(.05, .2),\n",
    "      corridor_width=2,\n",
    "      corridor_length=40,\n",
    "      aesthetic='outdoor_natural',\n",
    "      visible_side_planes=False)\n",
    "\n",
    "test.regenerate(random_state=None)\n",
    "\n",
    "# physics = mjcf.Physics.from_mjcf_model(test.mjcf_model)\n",
    "# PIL.Image.fromarray(physics.render())\n",
    "\n",
    "\n",
    "walker = ant.Ant(observable_options={'egocentric_camera': dict(enabled=True)})\n",
    "\n",
    "task = Task_Vnl(\n",
    "    walker=walker,\n",
    "    arena=test,\n",
    "    walker_spawn_position=(5, 0, 0),\n",
    "    walker_spawn_rotation=0,\n",
    "    target_velocity=1.0,\n",
    "    contact_termination=False,\n",
    "    terminate_at_height=-0.3)\n",
    "\n",
    "# env_composed = composer.Environment(time_limit=30,\n",
    "#                               task=task,\n",
    "#                               random_state=random_state,\n",
    "#                               strip_singleton_obs_buffer_dim=True)\n",
    "\n",
    "\n",
    "random_state = np.random.RandomState(12345)\n",
    "task.initialize_episode_mjcf(random_state)\n",
    "physics = mjcf.Physics.from_mjcf_model(task.root_entity.mjcf_model)\n",
    "#xml_package = task.root_entity.mjcf_model.to_xml_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewer.launch(environment_loader=env_composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walker Class Adapted\n",
    "Instead of directly getting the `Mujoco xml` or `mjcf` file, we get it directly by instantiating an customized `dm_control pymjcf` class and then extract it from there directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Walker(MjxEnv):\n",
    "  '''\n",
    "  This is greatly coustomizable of what reward you want to give: reward engineering\n",
    "  '''\n",
    "  def __init__(\n",
    "      self,\n",
    "      forward_reward_weight=1.25,\n",
    "      ctrl_cost_weight=0.1,\n",
    "      healthy_reward=5.0,\n",
    "      terminate_when_unhealthy=True,\n",
    "      healthy_z_range=(1.0, 1.5),\n",
    "      reset_noise_scale=1e-2,\n",
    "      exclude_current_positions_from_observation=True,\n",
    "      **kwargs,):\n",
    "    '''\n",
    "    Defining initilization of the agent\n",
    "    '''\n",
    "\n",
    "    mj_model = physics.model.ptr\n",
    "    # this is directly a mj_model already of type mujoco_py.MjModel (This is already a MJModel, same as previously in brax)\n",
    "    # the original xml load is directly creaing an new MjModel instance, which carries the configuration of everything, including mjtCone\n",
    "    # but this pass in one doesn't, it uses the default mjCONE_PYRAMIDAL, but MjModel now uses the eliptic model, so reset is needed\n",
    "\n",
    "    # solver is an optimization system\n",
    "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
    "    mj_model.opt.cone = mujoco.mjtCone.mjCONE_PYRAMIDAL # Read documentation\n",
    "\n",
    "    #Iterations for solver\n",
    "    mj_model.opt.iterations = 6\n",
    "    mj_model.opt.ls_iterations = 6\n",
    "\n",
    "    # Defult framne to be 5, but can self define in kwargs\n",
    "    physics_steps_per_control_step = 5\n",
    "    \n",
    "    kwargs['n_frames'] = kwargs.get(\n",
    "        'n_frames', physics_steps_per_control_step)\n",
    "\n",
    "    # Parents inheritence from MjxEnv class\n",
    "    super().__init__(model=mj_model, **kwargs)\n",
    "\n",
    "    # Global vraiable for later calling them\n",
    "    self._forward_reward_weight = forward_reward_weight\n",
    "    self._ctrl_cost_weight = ctrl_cost_weight\n",
    "    self._healthy_reward = healthy_reward\n",
    "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
    "    self._healthy_z_range = healthy_z_range\n",
    "    self._reset_noise_scale = reset_noise_scale\n",
    "    self._exclude_current_positions_from_observation = (exclude_current_positions_from_observation)\n",
    "\n",
    "  def reset(self, rng: jp.ndarray) -> State:\n",
    "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
    "\n",
    "    #Creating randome keys\n",
    "    #rng = random number generator key for starting random initiation\n",
    "    rng, rng1, rng2 = jax.random.split(rng, 3)\n",
    "\n",
    "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
    "\n",
    "    #Vectors of generalized joint position in the configuration space\n",
    "    qpos = self.sys.qpos0 + jax.random.uniform(\n",
    "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
    "    )\n",
    "\n",
    "    #Vectors of generalized joint velocities in the configuration space\n",
    "    qvel = jax.random.uniform(\n",
    "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
    "    )\n",
    "\n",
    "    data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "    #Reset everything\n",
    "    obs = self._get_obs(data.data, jp.zeros(self.sys.nu))\n",
    "    reward, done, zero = jp.zeros(3)\n",
    "    metrics = {\n",
    "        'forward_reward': zero,\n",
    "        'reward_linvel': zero,\n",
    "        'reward_quadctrl': zero,\n",
    "        'reward_alive': zero,\n",
    "        'x_position': zero,\n",
    "        'y_position': zero,\n",
    "        'distance_from_origin': zero,\n",
    "        'x_velocity': zero,\n",
    "        'y_velocity': zero,\n",
    "    }\n",
    "    return State(data, obs, reward, done, metrics)\n",
    "\n",
    "  def step(self, state: State, action: jp.ndarray) -> State:\n",
    "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
    "    #Previous Pipeline\n",
    "    data0 = state.pipeline_state\n",
    "\n",
    "    #Current pipeline state, step 1\n",
    "    data = self.pipeline_step(data0, action)\n",
    "\n",
    "    #Running forward (Velocity)\n",
    "    com_before = data0.data.subtree_com[1]\n",
    "    com_after = data.data.subtree_com[1]\n",
    "    velocity = (com_after - com_before) / self.dt\n",
    "    forward_reward = self._forward_reward_weight * velocity[0] * 2\n",
    "\n",
    "    #Height being healthy\n",
    "    min_z, max_z = self._healthy_z_range\n",
    "    is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
    "    is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
    "\n",
    "    #Termination condition\n",
    "    if self._terminate_when_unhealthy:\n",
    "      healthy_reward = self._healthy_reward\n",
    "    else:\n",
    "      healthy_reward = self._healthy_reward * is_healthy\n",
    "\n",
    "    #Control quad cost\n",
    "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
    "\n",
    "    #Feedback from env\n",
    "    obs = self._get_obs(data.data, action)\n",
    "    reward = forward_reward + healthy_reward - ctrl_cost\n",
    "\n",
    "    #Termination State\n",
    "    done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
    "\n",
    "    state.metrics.update(\n",
    "        forward_reward=forward_reward,\n",
    "        reward_linvel=forward_reward,\n",
    "        reward_quadctrl=-ctrl_cost,\n",
    "        reward_alive=healthy_reward,\n",
    "        x_position=com_after[0],\n",
    "        y_position=com_after[1],\n",
    "        distance_from_origin=jp.linalg.norm(com_after),\n",
    "        x_velocity=velocity[0],\n",
    "        y_velocity=velocity[1],\n",
    "    )\n",
    "\n",
    "    return state.replace(\n",
    "        pipeline_state=data, obs=obs, reward=reward, done=done\n",
    "    )\n",
    "\n",
    "  def _get_obs(self, data: mjx.Data, action: jp.ndarray) -> jp.ndarray:\n",
    "    \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
    "    position = data.qpos\n",
    "    if self._exclude_current_positions_from_observation:\n",
    "      position = position[2:]\n",
    "\n",
    "    # external_contact_forces are excluded\n",
    "    # environment observation described later\n",
    "    return jp.concatenate([\n",
    "        position,\n",
    "        data.qvel,\n",
    "        data.cinert[1:].ravel(),\n",
    "        data.cvel[1:].ravel(),\n",
    "        data.qfrc_actuator,\n",
    "    ])\n",
    "  \n",
    "# Registering the environment setup in env as humanoid_mjx\n",
    "envs.register_environment('walker', Walker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendoring a Rollout\n",
    "You can not render rollout on a non-linux machine as you need `ffmpeg`, which only exist on linux\n",
    "But the environment actually works -> establishes and step successfuly. You need to run this pipeline in:\n",
    "1. Linux Server\n",
    "2. Gpu based and having neccessary colab import and installation\n",
    "\n",
    "dm_control model did not implement `camera` need to find a substitute -> ask charles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = envs.get_environment(env_name='walker')\n",
    "\n",
    "# # define the jit reset/step functions\n",
    "# jit_reset = jax.jit(env.reset)\n",
    "# jit_step = jax.jit(env.step)\n",
    "\n",
    "# # initialize the state\n",
    "# state = jit_reset(jax.random.PRNGKey(0))\n",
    "\n",
    "# #Creating an container for rollout states\n",
    "# rollout = [state.pipeline_state]\n",
    "\n",
    "# # grab a trajectory\n",
    "# for i in (range(100)):\n",
    "#     ctrl = -0.1 * jp.ones(env.sys.nu)\n",
    "#     state = jit_step(state, ctrl)\n",
    "#     rollout.append(state.pipeline_state)\n",
    "\n",
    "# media.show_video(env.render(rollout), fps=1.0 / env.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
